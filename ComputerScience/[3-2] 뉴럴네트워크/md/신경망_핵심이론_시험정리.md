---
tags: [neural-network, deep-learning, exam, summary]
aliases: [신경망 시험 정리, NN Exam Notes]
created: 2025-10-18
---

# 신경망 핵심 이론 정리 (시험 대비)

> [!info] 문서 정보
> - 과목: 신경망 (Neural Network)
> - 목적: 시험 대비 핵심 개념 정리
> - 구성: 파이썬 기초 → 퍼셉트론 → 신경망 → 학습 알고리즘 → 오차역전파법

---

## 📚 목차

1. [[#Ch1. 파이썬 기초]]
2. [[#Ch2. 퍼셉트론 (Perceptron)]]
3. [[#Ch3. 신경망 (Neural Network)]]
4. [[#Ch4. 학습 알고리즘]]
5. [[#Ch5. 오차역전파법 (Backpropagation)]]
6. [[#핵심 수식 모음]]
7. [[#시험 출제 예상 문제]]

---

## Ch1. 파이썬 기초

### NumPy 핵심 개념

#### 브로드캐스팅 (Broadcasting)
```python
# 스칼라와 배열 연산
x = np.array([1, 2, 3])
x * 2  # [2, 4, 6] - 자동으로 모든 원소에 적용

# 다차원 배열 브로드캐스팅
A = np.array([[1, 2], [3, 4]])
B = np.array([10, 20])
A * B  # [[10, 40], [30, 80]] - B가 각 행에 적용됨
```

> [!tip] 암기 포인트
> - **브로드캐스팅**: 형상이 다른 배열 간 연산 시 자동으로 형상을 맞춰줌
> - **원소별 연산 (element-wise)**: `*`, `+`, `-`, `/` 등은 원소별로 계산
> - **행렬 곱**: `np.dot(A, B)` 사용 (내적)

#### NumPy 배열 연산 정리

| 연산 | 코드 | 설명 |
|------|------|------|
| 원소별 곱 | `A * B` | element-wise product |
| 행렬 곱 | `np.dot(A, B)` | matrix multiplication |
| 전치 행렬 | `A.T` 또는 `np.transpose(A)` | transpose |
| 평탄화 | `A.flatten()` | 1차원 배열로 변환 |

---

## Ch2. 퍼셉트론 (Perceptron)

### 퍼셉트론의 정의

> [!important] 핵심 개념
> **퍼셉트론**: 다수의 신호를 입력받아 하나의 신호를 출력하는 알고리즘
> - 입력: $x_1, x_2, ..., x_n$
> - 가중치: $w_1, w_2, ..., w_n$
> - 편향: $b$
> - 출력: $y = \begin{cases} 0 & (w_1x_1 + w_2x_2 + b \leq 0) \\ 1 & (w_1x_1 + w_2x_2 + b > 0) \end{cases}$

### 가중치와 편향

- **가중치 (weight, w)**: 각 입력 신호의 중요도를 나타냄
  - 가중치가 클수록 해당 입력이 중요함
  
- **편향 (bias, b)**: 뉴런이 얼마나 쉽게 활성화되는지 조정
  - 편향이 클수록 쉽게 활성화됨
  - 편향이 작을수록 활성화 어려움

### 논리 게이트 구현

#### AND 게이트
```python
def AND(x1, x2):
    w1, w2, b = 0.5, 0.5, -0.7
    tmp = w1*x1 + w2*x2 + b
    return 1 if tmp > 0 else 0
```

| x1 | x2 | y |
|----|----|---|
| 0  | 0  | 0 |
| 0  | 1  | 0 |
| 1  | 0  | 0 |
| 1  | 1  | 1 |

#### OR 게이트
```python
def OR(x1, x2):
    w1, w2, b = 0.5, 0.5, -0.2  # 편향이 AND보다 큼
    tmp = w1*x1 + w2*x2 + b
    return 1 if tmp > 0 else 0
```

#### NAND 게이트 (NOT AND)
```python
def NAND(x1, x2):
    w1, w2, b = -0.5, -0.5, 0.7  # 가중치가 음수
    tmp = w1*x1 + w2*x2 + b
    return 1 if tmp > 0 else 0
```

### 다층 퍼셉트론 (Multi-Layer Perceptron)

> [!warning] 중요 개념
> **XOR 게이트는 단층 퍼셉트론으로 구현 불가능**
> - 이유: 선형 분리 불가능 (비선형 문제)
> - 해결: 다층 퍼셉트론 필요

#### XOR 게이트 (2층 구조)
```python
def XOR(x1, x2):
    s1 = NAND(x1, x2)  # 은닉층 1
    s2 = OR(x1, x2)    # 은닉층 2
    y = AND(s1, s2)    # 출력층
    return y
```

**XOR 진리표**:

| x1 | x2 | NAND | OR | AND (출력) |
|----|----|------|----|----|
| 0  | 0  | 1    | 0  | 0  |
| 0  | 1  | 1    | 1  | 1  |
| 1  | 0  | 1    | 1  | 1  |
| 1  | 1  | 0    | 1  | 0  |

---

## Ch3. 신경망 (Neural Network)

### 활성화 함수 (Activation Function)

> [!important] 왜 활성화 함수가 필요한가?
> - 선형 함수만 사용하면 층을 아무리 깊게 해도 단층과 동일
> - 비선형 함수를 사용해야 층을 쌓는 의미가 있음

#### 1. 계단 함수 (Step Function)
```python
def step_function(x):
    return np.array(x > 0, dtype=int)
```
- 임계값을 기준으로 0 또는 1 출력
- **단점**: 미분 불가능 → 학습 불가

#### 2. 시그모이드 함수 (Sigmoid Function)
$$\sigma(x) = \frac{1}{1 + e^{-x}}$$

```python
def sigmoid(x):
    return 1 / (1 + np.exp(-x))
```

**특징**:
- 출력 범위: (0, 1)
- 매끄러운 곡선
- 미분 가능
- **단점**: 기울기 소실 문제 (gradient vanishing)

**미분**:
$$\sigma'(x) = \sigma(x)(1 - \sigma(x))$$

#### 3. ReLU 함수 (Rectified Linear Unit)
$$\text{ReLU}(x) = \max(0, x)$$

```python
def relu(x):
    return np.maximum(0, x)
```

**특징**:
- $x > 0$: 출력 = x (기울기 1)
- $x \leq 0$: 출력 = 0 (기울기 0)
- **장점**: 
  - 기울기 소실 문제 해결
  - 계산 효율적
  - 현재 가장 많이 사용됨

**미분**:
$$\text{ReLU}'(x) = \begin{cases} 1 & (x > 0) \\ 0 & (x \leq 0) \end{cases}$$

### 출력층 활성화 함수

#### 소프트맥스 함수 (Softmax)
$$y_k = \frac{\exp(a_k)}{\sum_{i=1}^{n} \exp(a_i)}$$

```python
def softmax(x):
    c = np.max(x)  # 오버플로 방지
    exp_x = np.exp(x - c)
    return exp_x / np.sum(exp_x)
```

**특징**:
- 다중 클래스 분류에 사용
- 출력의 합이 1 (확률 분포)
- 오버플로 방지를 위해 최댓값을 빼줌

> [!tip] 출력층 활성화 함수 선택
> - **회귀 문제**: 항등 함수 (identity function)
> - **이진 분류**: 시그모이드
> - **다중 분류**: 소프트맥스

---

## Ch4. 학습 알고리즘

### 손실 함수 (Loss Function)

> [!important] 손실 함수의 역할
> - 신경망의 성능을 "나쁨의 정도"로 측정
> - 학습의 목표: 손실 함수를 최소화하는 가중치 찾기
> - 정확도가 아닌 손실 함수를 사용하는 이유: 미분 가능해야 함

#### 1. 평균 제곱 오차 (MSE, Mean Squared Error)
$$E = \frac{1}{2}\sum_{k}(y_k - t_k)^2$$

```python
def mean_squared_error(y, t):
    return 0.5 * np.sum((y - t)**2)
```

- 주로 **회귀 문제**에 사용
- $y$: 신경망 출력, $t$: 정답 레이블

#### 2. 교차 엔트로피 오차 (CEE, Cross Entropy Error)
$$E = -\sum_{k} t_k \log y_k$$

```python
def cross_entropy_error(y, t):
    delta = 1e-7  # log(0) 방지
    return -np.sum(t * np.log(y + delta))
```

- 주로 **분류 문제**에 사용
- $t$: 원-핫 인코딩된 정답 레이블
- $y$: 소프트맥스 출력 (확률)

> [!warning] 시험 출제 포인트
> - MSE는 회귀, CEE는 분류
> - CEE에서 `delta`를 더하는 이유: log(0) = -∞ 방지
> - 원-핫 인코딩: [0, 0, 1, 0, 0] 형태

### 미니배치 학습

```python
train_size = x_train.shape[0]
batch_size = 100
batch_mask = np.random.choice(train_size, batch_size)
x_batch = x_train[batch_mask]
t_batch = t_train[batch_mask]
```

- 전체 데이터가 아닌 일부(미니배치)만 사용
- **장점**: 메모리 효율적, 학습 속도 향상

### 경사 하강법 (Gradient Descent)

$$W \leftarrow W - \eta \frac{\partial L}{\partial W}$$

- $W$: 가중치
- $\eta$: 학습률 (learning rate)
- $\frac{\partial L}{\partial W}$: 손실 함수의 기울기

```python
def gradient_descent(f, init_x, lr=0.01, step_num=100):
    x = init_x
    for i in range(step_num):
        grad = numerical_gradient(f, x)
        x -= lr * grad  # 가중치 갱신
    return x
```

> [!important] 학습률 설정
> - **너무 크면**: 발산 (학습 불안정)
> - **너무 작으면**: 학습 매우 느림
> - **적절한 값**: 0.01 ~ 0.001 정도

### 수치 미분 vs 해석적 미분

#### 수치 미분 (Numerical Differentiation)
$$\frac{df(x)}{dx} \approx \frac{f(x+h) - f(x-h)}{2h}$$

```python
def numerical_gradient(f, x):
    h = 1e-4  # 0.0001
    grad = np.zeros_like(x)
    
    for idx in range(x.size):
        tmp_val = x[idx]
        # f(x+h)
        x[idx] = tmp_val + h
        fxh1 = f(x)
        # f(x-h)
        x[idx] = tmp_val - h
        fxh2 = f(x)
        # 중심 차분법
        grad[idx] = (fxh1 - fxh2) / (2*h)
        x[idx] = tmp_val
    
    return grad
```

- **장점**: 구현 간단, 정확
- **단점**: 계산 느림
- **용도**: 오차역전파법 검증 (gradient check)

---

## Ch5. 오차역전파법 (Backpropagation)

### 계산 그래프

> [!important] 계산 그래프 개념
> - 계산 과정을 그래프로 표현
> - **순전파 (Forward)**: 입력 → 출력 계산
> - **역전파 (Backward)**: 미분(기울기) 계산

### 연쇄 법칙 (Chain Rule)

$$\frac{\partial z}{\partial x} = \frac{\partial z}{\partial y} \cdot \frac{\partial y}{\partial x}$$

- 합성 함수의 미분은 각 함수 미분의 곱

### 주요 계층 (Layer) 구현

#### 1. 곱셈 계층 (Mul Layer)
```python
class MulLayer:
    def __init__(self):
        self.x = None
        self.y = None
    
    def forward(self, x, y):
        self.x = x
        self.y = y
        return x * y
    
    def backward(self, dout):
        dx = dout * self.y  # 순전파 시 y를 곱했으므로
        dy = dout * self.x  # 순전파 시 x를 곱했으므로
        return dx, dy
```

**미분 규칙**:
- $z = xy$일 때
- $\frac{\partial z}{\partial x} = y$
- $\frac{\partial z}{\partial y} = x$

#### 2. 덧셈 계층 (Add Layer)
```python
class AddLayer:
    def forward(self, x, y):
        return x + y
    
    def backward(self, dout):
        dx = dout * 1  # 덧셈의 미분은 1
        dy = dout * 1
        return dx, dy
```

#### 3. ReLU 계층
```python
class Relu:
    def __init__(self):
        self.mask = None  # True/False 배열
    
    def forward(self, x):
        self.mask = (x <= 0)  # 0 이하인 원소 위치 저장
        out = x.copy()
        out[self.mask] = 0
        return out
    
    def backward(self, dout):
        dout[self.mask] = 0  # 0 이하였던 곳은 기울기 0
        return dout
```

#### 4. Sigmoid 계층
```python
class Sigmoid:
    def __init__(self):
        self.out = None
    
    def forward(self, x):
        out = 1 / (1 + np.exp(-x))
        self.out = out
        return out
    
    def backward(self, dout):
        dx = dout * self.out * (1.0 - self.out)
        return dx
```

**시그모이드 미분 공식**:
$$\sigma'(x) = \sigma(x)(1 - \sigma(x))$$

#### 5. Affine 계층 (완전연결층)
```python
class Affine:
    def __init__(self, W, b):
        self.W = W
        self.b = b
        self.x = None
        self.dW = None
        self.db = None
    
    def forward(self, x):
        self.x = x
        out = np.dot(x, self.W) + self.b
        return out
    
    def backward(self, dout):
        dx = np.dot(dout, self.W.T)
        self.dW = np.dot(self.x.T, dout)
        self.db = np.sum(dout, axis=0)
        return dx
```

**행렬 미분 규칙**:
- $Y = XW + B$일 때
- $\frac{\partial L}{\partial X} = \frac{\partial L}{\partial Y} \cdot W^T$
- $\frac{\partial L}{\partial W} = X^T \cdot \frac{\partial L}{\partial Y}$

#### 6. Softmax-with-Loss 계층
```python
class SoftmaxWithLoss:
    def __init__(self):
        self.loss = None
        self.y = None  # softmax 출력
        self.t = None  # 정답 레이블
    
    def forward(self, x, t):
        self.t = t
        self.y = softmax(x)
        self.loss = cross_entropy_error(self.y, self.t)
        return self.loss
    
    def backward(self, dout=1):
        batch_size = self.t.shape[0]
        dx = (self.y - self.t) / batch_size
        return dx
```

> [!tip] 중요한 성질
> Softmax + Cross Entropy의 역전파는 매우 간단!
> $$\frac{\partial L}{\partial a} = y - t$$
> - $y$: softmax 출력
> - $t$: 정답 레이블 (원-핫)

---

## 핵심 수식 모음

### 활성화 함수

| 함수 | 수식 | 미분 |
|------|------|------|
| Sigmoid | $\sigma(x) = \frac{1}{1+e^{-x}}$ | $\sigma'(x) = \sigma(x)(1-\sigma(x))$ |
| ReLU | $\max(0, x)$ | $\begin{cases} 1 & (x>0) \\ 0 & (x\leq0) \end{cases}$ |
| Softmax | $y_k = \frac{e^{a_k}}{\sum_i e^{a_i}}$ | - |

### 손실 함수

| 함수 | 수식 | 용도 |
|------|------|------|
| MSE | $E = \frac{1}{2}\sum(y-t)^2$ | 회귀 |
| CEE | $E = -\sum t \log y$ | 분류 |

### 경사 하강법

$$W \leftarrow W - \eta \frac{\partial L}{\partial W}$$

### 오차역전파 핵심

$$\frac{\partial L}{\partial x} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial x}$$

---

## 시험 출제 예상 문제

### 1. 개념 문제

> [!question] Q1. 퍼셉트론과 신경망의 차이점은?
> **답**: 
> - 퍼셉트론: 단층 구조, 선형 분리 가능한 문제만 해결
> - 신경망: 다층 구조, 비선형 활성화 함수 사용, 복잡한 문제 해결 가능

> [!question] Q2. 왜 활성화 함수는 비선형 함수여야 하는가?
> **답**: 선형 함수만 사용하면 층을 아무리 깊게 해도 하나의 선형 함수로 표현 가능. 비선형 함수를 사용해야 층을 쌓는 의미가 있음.

> [!question] Q3. 학습 시 정확도가 아닌 손실 함수를 사용하는 이유는?
> **답**: 정확도는 대부분의 장소에서 미분값이 0이 되어 매개변수를 갱신할 수 없음. 손실 함수는 연속적이고 미분 가능하여 경사하강법 적용 가능.

### 2. 계산 문제

> [!question] Q4. AND 게이트의 가중치와 편향을 설정하시오.
> **답**: 
> ```python
> w1 = 0.5, w2 = 0.5, b = -0.7
> # 또는 다른 값도 가능 (조건: w1*1 + w2*1 + b > 0)
> ```

> [!question] Q5. 시그모이드 함수 σ(x) = 1/(1+e^(-x))의 미분을 구하시오.
> **답**: σ'(x) = σ(x)(1 - σ(x))

### 3. 구현 문제

> [!question] Q6. ReLU 함수를 NumPy로 구현하시오.
> **답**:
> ```python
> def relu(x):
>     return np.maximum(0, x)
> ```

> [!question] Q7. 교차 엔트로피 오차 함수를 구현하시오.
> **답**:
> ```python
> def cross_entropy_error(y, t):
>     delta = 1e-7
>     return -np.sum(t * np.log(y + delta))
> ```

### 4. 오차역전파 문제

> [!question] Q8. 곱셈 계층의 역전파를 설명하시오.
> **답**:
> - 순전파: z = x * y
> - 역전파: 
>   - ∂z/∂x = y → dx = dout * y
>   - ∂z/∂y = x → dy = dout * x

> [!question] Q9. Affine 계층에서 가중치 W의 기울기를 구하는 식은?
> **답**: dW = X^T · dout

---

## 핵심 암기 사항

> [!important] 반드시 암기!
> 
> ### 활성화 함수 선택
> - **은닉층**: ReLU (현재 표준)
> - **출력층 (회귀)**: 항등 함수
> - **출력층 (이진 분류)**: Sigmoid
> - **출력층 (다중 분류)**: Softmax
> 
> ### 손실 함수 선택
> - **회귀 문제**: MSE (평균 제곱 오차)
> - **분류 문제**: CEE (교차 엔트로피 오차)
> 
> ### 학습 과정
> 1. 미니배치: 훈련 데이터 중 일부를 무작위로 선택
> 2. 기울기 계산: 손실 함수의 미분 계산
> 3. 매개변수 갱신: W ← W - η·∇W
> 4. 반복
> 
> ### 오차역전파법 장점
> - 수치 미분보다 훨씬 빠름
> - 매개변수가 많아도 효율적으로 계산 가능

---

## 추가 학습 자료

- [[Ch2. 퍼셉트론 상세 정리]]
- [[Ch4. 경사 하강법 최적화]]
- [[Ch5. 오차역전파법 수학적 증명]]

---

> [!success] 시험 공부 체크리스트
> - [ ] 각 활성화 함수의 특징과 미분 공식 암기
> - [ ] 손실 함수 공식과 용도 암기
> - [ ] 퍼셉트론으로 XOR를 구현할 수 없는 이유 이해
> - [ ] 경사 하강법 수식 암기
> - [ ] 주요 계층의 순전파/역전파 구현 이해
> - [ ] Softmax-with-Loss의 역전파가 y-t인 이유 이해
