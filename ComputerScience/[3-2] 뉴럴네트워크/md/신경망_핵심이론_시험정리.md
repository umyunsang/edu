---
tags: [neural-network, deep-learning, exam, summary]
aliases: [ì‹ ê²½ë§ ì‹œí—˜ ì •ë¦¬, NN Exam Notes]
created: 2025-10-18
---

# ì‹ ê²½ë§ í•µì‹¬ ì´ë¡  ì •ë¦¬ (ì‹œí—˜ ëŒ€ë¹„)

> [!info] ë¬¸ì„œ ì •ë³´
> - ê³¼ëª©: ì‹ ê²½ë§ (Neural Network)
> - ëª©ì : ì‹œí—˜ ëŒ€ë¹„ í•µì‹¬ ê°œë… ì •ë¦¬
> - êµ¬ì„±: íŒŒì´ì¬ ê¸°ì´ˆ â†’ í¼ì…‰íŠ¸ë¡  â†’ ì‹ ê²½ë§ â†’ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ â†’ ì˜¤ì°¨ì—­ì „íŒŒë²•

---

## ğŸ“š ëª©ì°¨

1. [[#Ch1. íŒŒì´ì¬ ê¸°ì´ˆ]]
2. [[#Ch2. í¼ì…‰íŠ¸ë¡  (Perceptron)]]
3. [[#Ch3. ì‹ ê²½ë§ (Neural Network)]]
4. [[#Ch4. í•™ìŠµ ì•Œê³ ë¦¬ì¦˜]]
5. [[#Ch5. ì˜¤ì°¨ì—­ì „íŒŒë²• (Backpropagation)]]
6. [[#í•µì‹¬ ìˆ˜ì‹ ëª¨ìŒ]]
7. [[#ì‹œí—˜ ì¶œì œ ì˜ˆìƒ ë¬¸ì œ]]

---

## Ch1. íŒŒì´ì¬ ê¸°ì´ˆ

### NumPy í•µì‹¬ ê°œë…

#### ë¸Œë¡œë“œìºìŠ¤íŒ… (Broadcasting)
```python
# ìŠ¤ì¹¼ë¼ì™€ ë°°ì—´ ì—°ì‚°
x = np.array([1, 2, 3])
x * 2  # [2, 4, 6] - ìë™ìœ¼ë¡œ ëª¨ë“  ì›ì†Œì— ì ìš©

# ë‹¤ì°¨ì› ë°°ì—´ ë¸Œë¡œë“œìºìŠ¤íŒ…
A = np.array([[1, 2], [3, 4]])
B = np.array([10, 20])
A * B  # [[10, 40], [30, 80]] - Bê°€ ê° í–‰ì— ì ìš©ë¨
```

> [!tip] ì•”ê¸° í¬ì¸íŠ¸
> - **ë¸Œë¡œë“œìºìŠ¤íŒ…**: í˜•ìƒì´ ë‹¤ë¥¸ ë°°ì—´ ê°„ ì—°ì‚° ì‹œ ìë™ìœ¼ë¡œ í˜•ìƒì„ ë§ì¶°ì¤Œ
> - **ì›ì†Œë³„ ì—°ì‚° (element-wise)**: `*`, `+`, `-`, `/` ë“±ì€ ì›ì†Œë³„ë¡œ ê³„ì‚°
> - **í–‰ë ¬ ê³±**: `np.dot(A, B)` ì‚¬ìš© (ë‚´ì )

#### NumPy ë°°ì—´ ì—°ì‚° ì •ë¦¬

| ì—°ì‚° | ì½”ë“œ | ì„¤ëª… |
|------|------|------|
| ì›ì†Œë³„ ê³± | `A * B` | element-wise product |
| í–‰ë ¬ ê³± | `np.dot(A, B)` | matrix multiplication |
| ì „ì¹˜ í–‰ë ¬ | `A.T` ë˜ëŠ” `np.transpose(A)` | transpose |
| í‰íƒ„í™” | `A.flatten()` | 1ì°¨ì› ë°°ì—´ë¡œ ë³€í™˜ |

---

## Ch2. í¼ì…‰íŠ¸ë¡  (Perceptron)

### í¼ì…‰íŠ¸ë¡ ì˜ ì •ì˜

> [!important] í•µì‹¬ ê°œë…
> **í¼ì…‰íŠ¸ë¡ **: ë‹¤ìˆ˜ì˜ ì‹ í˜¸ë¥¼ ì…ë ¥ë°›ì•„ í•˜ë‚˜ì˜ ì‹ í˜¸ë¥¼ ì¶œë ¥í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜
> - ì…ë ¥: $x_1, x_2, ..., x_n$
> - ê°€ì¤‘ì¹˜: $w_1, w_2, ..., w_n$
> - í¸í–¥: $b$
> - ì¶œë ¥: $y = \begin{cases} 0 & (w_1x_1 + w_2x_2 + b \leq 0) \\ 1 & (w_1x_1 + w_2x_2 + b > 0) \end{cases}$

### ê°€ì¤‘ì¹˜ì™€ í¸í–¥

- **ê°€ì¤‘ì¹˜ (weight, w)**: ê° ì…ë ¥ ì‹ í˜¸ì˜ ì¤‘ìš”ë„ë¥¼ ë‚˜íƒ€ëƒ„
  - ê°€ì¤‘ì¹˜ê°€ í´ìˆ˜ë¡ í•´ë‹¹ ì…ë ¥ì´ ì¤‘ìš”í•¨
  
- **í¸í–¥ (bias, b)**: ë‰´ëŸ°ì´ ì–¼ë§ˆë‚˜ ì‰½ê²Œ í™œì„±í™”ë˜ëŠ”ì§€ ì¡°ì •
  - í¸í–¥ì´ í´ìˆ˜ë¡ ì‰½ê²Œ í™œì„±í™”ë¨
  - í¸í–¥ì´ ì‘ì„ìˆ˜ë¡ í™œì„±í™” ì–´ë ¤ì›€

### ë…¼ë¦¬ ê²Œì´íŠ¸ êµ¬í˜„

#### AND ê²Œì´íŠ¸
```python
def AND(x1, x2):
    w1, w2, b = 0.5, 0.5, -0.7
    tmp = w1*x1 + w2*x2 + b
    return 1 if tmp > 0 else 0
```

| x1 | x2 | y |
|----|----|---|
| 0  | 0  | 0 |
| 0  | 1  | 0 |
| 1  | 0  | 0 |
| 1  | 1  | 1 |

#### OR ê²Œì´íŠ¸
```python
def OR(x1, x2):
    w1, w2, b = 0.5, 0.5, -0.2  # í¸í–¥ì´ ANDë³´ë‹¤ í¼
    tmp = w1*x1 + w2*x2 + b
    return 1 if tmp > 0 else 0
```

#### NAND ê²Œì´íŠ¸ (NOT AND)
```python
def NAND(x1, x2):
    w1, w2, b = -0.5, -0.5, 0.7  # ê°€ì¤‘ì¹˜ê°€ ìŒìˆ˜
    tmp = w1*x1 + w2*x2 + b
    return 1 if tmp > 0 else 0
```

### ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡  (Multi-Layer Perceptron)

> [!warning] ì¤‘ìš” ê°œë…
> **XOR ê²Œì´íŠ¸ëŠ” ë‹¨ì¸µ í¼ì…‰íŠ¸ë¡ ìœ¼ë¡œ êµ¬í˜„ ë¶ˆê°€ëŠ¥**
> - ì´ìœ : ì„ í˜• ë¶„ë¦¬ ë¶ˆê°€ëŠ¥ (ë¹„ì„ í˜• ë¬¸ì œ)
> - í•´ê²°: ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡  í•„ìš”

#### XOR ê²Œì´íŠ¸ (2ì¸µ êµ¬ì¡°)
```python
def XOR(x1, x2):
    s1 = NAND(x1, x2)  # ì€ë‹‰ì¸µ 1
    s2 = OR(x1, x2)    # ì€ë‹‰ì¸µ 2
    y = AND(s1, s2)    # ì¶œë ¥ì¸µ
    return y
```

**XOR ì§„ë¦¬í‘œ**:

| x1 | x2 | NAND | OR | AND (ì¶œë ¥) |
|----|----|------|----|----|
| 0  | 0  | 1    | 0  | 0  |
| 0  | 1  | 1    | 1  | 1  |
| 1  | 0  | 1    | 1  | 1  |
| 1  | 1  | 0    | 1  | 0  |

---

## Ch3. ì‹ ê²½ë§ (Neural Network)

### í™œì„±í™” í•¨ìˆ˜ (Activation Function)

> [!important] ì™œ í™œì„±í™” í•¨ìˆ˜ê°€ í•„ìš”í•œê°€?
> - ì„ í˜• í•¨ìˆ˜ë§Œ ì‚¬ìš©í•˜ë©´ ì¸µì„ ì•„ë¬´ë¦¬ ê¹Šê²Œ í•´ë„ ë‹¨ì¸µê³¼ ë™ì¼
> - ë¹„ì„ í˜• í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ì•¼ ì¸µì„ ìŒ“ëŠ” ì˜ë¯¸ê°€ ìˆìŒ

#### 1. ê³„ë‹¨ í•¨ìˆ˜ (Step Function)
```python
def step_function(x):
    return np.array(x > 0, dtype=int)
```
- ì„ê³„ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ 0 ë˜ëŠ” 1 ì¶œë ¥
- **ë‹¨ì **: ë¯¸ë¶„ ë¶ˆê°€ëŠ¥ â†’ í•™ìŠµ ë¶ˆê°€

#### 2. ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ (Sigmoid Function)
$$\sigma(x) = \frac{1}{1 + e^{-x}}$$

```python
def sigmoid(x):
    return 1 / (1 + np.exp(-x))
```

**íŠ¹ì§•**:
- ì¶œë ¥ ë²”ìœ„: (0, 1)
- ë§¤ë„ëŸ¬ìš´ ê³¡ì„ 
- ë¯¸ë¶„ ê°€ëŠ¥
- **ë‹¨ì **: ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œ (gradient vanishing)

**ë¯¸ë¶„**:
$$\sigma'(x) = \sigma(x)(1 - \sigma(x))$$

#### 3. ReLU í•¨ìˆ˜ (Rectified Linear Unit)
$$\text{ReLU}(x) = \max(0, x)$$

```python
def relu(x):
    return np.maximum(0, x)
```

**íŠ¹ì§•**:
- $x > 0$: ì¶œë ¥ = x (ê¸°ìš¸ê¸° 1)
- $x \leq 0$: ì¶œë ¥ = 0 (ê¸°ìš¸ê¸° 0)
- **ì¥ì **: 
  - ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œ í•´ê²°
  - ê³„ì‚° íš¨ìœ¨ì 
  - í˜„ì¬ ê°€ì¥ ë§ì´ ì‚¬ìš©ë¨

**ë¯¸ë¶„**:
$$\text{ReLU}'(x) = \begin{cases} 1 & (x > 0) \\ 0 & (x \leq 0) \end{cases}$$

### ì¶œë ¥ì¸µ í™œì„±í™” í•¨ìˆ˜

#### ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ (Softmax)
$$y_k = \frac{\exp(a_k)}{\sum_{i=1}^{n} \exp(a_i)}$$

```python
def softmax(x):
    c = np.max(x)  # ì˜¤ë²„í”Œë¡œ ë°©ì§€
    exp_x = np.exp(x - c)
    return exp_x / np.sum(exp_x)
```

**íŠ¹ì§•**:
- ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ì— ì‚¬ìš©
- ì¶œë ¥ì˜ í•©ì´ 1 (í™•ë¥  ë¶„í¬)
- ì˜¤ë²„í”Œë¡œ ë°©ì§€ë¥¼ ìœ„í•´ ìµœëŒ“ê°’ì„ ë¹¼ì¤Œ

> [!tip] ì¶œë ¥ì¸µ í™œì„±í™” í•¨ìˆ˜ ì„ íƒ
> - **íšŒê·€ ë¬¸ì œ**: í•­ë“± í•¨ìˆ˜ (identity function)
> - **ì´ì§„ ë¶„ë¥˜**: ì‹œê·¸ëª¨ì´ë“œ
> - **ë‹¤ì¤‘ ë¶„ë¥˜**: ì†Œí”„íŠ¸ë§¥ìŠ¤

---

## Ch4. í•™ìŠµ ì•Œê³ ë¦¬ì¦˜

### ì†ì‹¤ í•¨ìˆ˜ (Loss Function)

> [!important] ì†ì‹¤ í•¨ìˆ˜ì˜ ì—­í• 
> - ì‹ ê²½ë§ì˜ ì„±ëŠ¥ì„ "ë‚˜ì¨ì˜ ì •ë„"ë¡œ ì¸¡ì •
> - í•™ìŠµì˜ ëª©í‘œ: ì†ì‹¤ í•¨ìˆ˜ë¥¼ ìµœì†Œí™”í•˜ëŠ” ê°€ì¤‘ì¹˜ ì°¾ê¸°
> - ì •í™•ë„ê°€ ì•„ë‹Œ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ëŠ” ì´ìœ : ë¯¸ë¶„ ê°€ëŠ¥í•´ì•¼ í•¨

#### 1. í‰ê·  ì œê³± ì˜¤ì°¨ (MSE, Mean Squared Error)
$$E = \frac{1}{2}\sum_{k}(y_k - t_k)^2$$

```python
def mean_squared_error(y, t):
    return 0.5 * np.sum((y - t)**2)
```

- ì£¼ë¡œ **íšŒê·€ ë¬¸ì œ**ì— ì‚¬ìš©
- $y$: ì‹ ê²½ë§ ì¶œë ¥, $t$: ì •ë‹µ ë ˆì´ë¸”

#### 2. êµì°¨ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨ (CEE, Cross Entropy Error)
$$E = -\sum_{k} t_k \log y_k$$

```python
def cross_entropy_error(y, t):
    delta = 1e-7  # log(0) ë°©ì§€
    return -np.sum(t * np.log(y + delta))
```

- ì£¼ë¡œ **ë¶„ë¥˜ ë¬¸ì œ**ì— ì‚¬ìš©
- $t$: ì›-í•« ì¸ì½”ë”©ëœ ì •ë‹µ ë ˆì´ë¸”
- $y$: ì†Œí”„íŠ¸ë§¥ìŠ¤ ì¶œë ¥ (í™•ë¥ )

> [!warning] ì‹œí—˜ ì¶œì œ í¬ì¸íŠ¸
> - MSEëŠ” íšŒê·€, CEEëŠ” ë¶„ë¥˜
> - CEEì—ì„œ `delta`ë¥¼ ë”í•˜ëŠ” ì´ìœ : log(0) = -âˆ ë°©ì§€
> - ì›-í•« ì¸ì½”ë”©: [0, 0, 1, 0, 0] í˜•íƒœ

### ë¯¸ë‹ˆë°°ì¹˜ í•™ìŠµ

```python
train_size = x_train.shape[0]
batch_size = 100
batch_mask = np.random.choice(train_size, batch_size)
x_batch = x_train[batch_mask]
t_batch = t_train[batch_mask]
```

- ì „ì²´ ë°ì´í„°ê°€ ì•„ë‹Œ ì¼ë¶€(ë¯¸ë‹ˆë°°ì¹˜)ë§Œ ì‚¬ìš©
- **ì¥ì **: ë©”ëª¨ë¦¬ íš¨ìœ¨ì , í•™ìŠµ ì†ë„ í–¥ìƒ

### ê²½ì‚¬ í•˜ê°•ë²• (Gradient Descent)

$$W \leftarrow W - \eta \frac{\partial L}{\partial W}$$

- $W$: ê°€ì¤‘ì¹˜
- $\eta$: í•™ìŠµë¥  (learning rate)
- $\frac{\partial L}{\partial W}$: ì†ì‹¤ í•¨ìˆ˜ì˜ ê¸°ìš¸ê¸°

```python
def gradient_descent(f, init_x, lr=0.01, step_num=100):
    x = init_x
    for i in range(step_num):
        grad = numerical_gradient(f, x)
        x -= lr * grad  # ê°€ì¤‘ì¹˜ ê°±ì‹ 
    return x
```

> [!important] í•™ìŠµë¥  ì„¤ì •
> - **ë„ˆë¬´ í¬ë©´**: ë°œì‚° (í•™ìŠµ ë¶ˆì•ˆì •)
> - **ë„ˆë¬´ ì‘ìœ¼ë©´**: í•™ìŠµ ë§¤ìš° ëŠë¦¼
> - **ì ì ˆí•œ ê°’**: 0.01 ~ 0.001 ì •ë„

### ìˆ˜ì¹˜ ë¯¸ë¶„ vs í•´ì„ì  ë¯¸ë¶„

#### ìˆ˜ì¹˜ ë¯¸ë¶„ (Numerical Differentiation)
$$\frac{df(x)}{dx} \approx \frac{f(x+h) - f(x-h)}{2h}$$

```python
def numerical_gradient(f, x):
    h = 1e-4  # 0.0001
    grad = np.zeros_like(x)
    
    for idx in range(x.size):
        tmp_val = x[idx]
        # f(x+h)
        x[idx] = tmp_val + h
        fxh1 = f(x)
        # f(x-h)
        x[idx] = tmp_val - h
        fxh2 = f(x)
        # ì¤‘ì‹¬ ì°¨ë¶„ë²•
        grad[idx] = (fxh1 - fxh2) / (2*h)
        x[idx] = tmp_val
    
    return grad
```

- **ì¥ì **: êµ¬í˜„ ê°„ë‹¨, ì •í™•
- **ë‹¨ì **: ê³„ì‚° ëŠë¦¼
- **ìš©ë„**: ì˜¤ì°¨ì—­ì „íŒŒë²• ê²€ì¦ (gradient check)

---

## Ch5. ì˜¤ì°¨ì—­ì „íŒŒë²• (Backpropagation)

### ê³„ì‚° ê·¸ë˜í”„

> [!important] ê³„ì‚° ê·¸ë˜í”„ ê°œë…
> - ê³„ì‚° ê³¼ì •ì„ ê·¸ë˜í”„ë¡œ í‘œí˜„
> - **ìˆœì „íŒŒ (Forward)**: ì…ë ¥ â†’ ì¶œë ¥ ê³„ì‚°
> - **ì—­ì „íŒŒ (Backward)**: ë¯¸ë¶„(ê¸°ìš¸ê¸°) ê³„ì‚°

### ì—°ì‡„ ë²•ì¹™ (Chain Rule)

$$\frac{\partial z}{\partial x} = \frac{\partial z}{\partial y} \cdot \frac{\partial y}{\partial x}$$

- í•©ì„± í•¨ìˆ˜ì˜ ë¯¸ë¶„ì€ ê° í•¨ìˆ˜ ë¯¸ë¶„ì˜ ê³±

### ì£¼ìš” ê³„ì¸µ (Layer) êµ¬í˜„

#### 1. ê³±ì…ˆ ê³„ì¸µ (Mul Layer)
```python
class MulLayer:
    def __init__(self):
        self.x = None
        self.y = None
    
    def forward(self, x, y):
        self.x = x
        self.y = y
        return x * y
    
    def backward(self, dout):
        dx = dout * self.y  # ìˆœì „íŒŒ ì‹œ yë¥¼ ê³±í–ˆìœ¼ë¯€ë¡œ
        dy = dout * self.x  # ìˆœì „íŒŒ ì‹œ xë¥¼ ê³±í–ˆìœ¼ë¯€ë¡œ
        return dx, dy
```

**ë¯¸ë¶„ ê·œì¹™**:
- $z = xy$ì¼ ë•Œ
- $\frac{\partial z}{\partial x} = y$
- $\frac{\partial z}{\partial y} = x$

#### 2. ë§ì…ˆ ê³„ì¸µ (Add Layer)
```python
class AddLayer:
    def forward(self, x, y):
        return x + y
    
    def backward(self, dout):
        dx = dout * 1  # ë§ì…ˆì˜ ë¯¸ë¶„ì€ 1
        dy = dout * 1
        return dx, dy
```

#### 3. ReLU ê³„ì¸µ
```python
class Relu:
    def __init__(self):
        self.mask = None  # True/False ë°°ì—´
    
    def forward(self, x):
        self.mask = (x <= 0)  # 0 ì´í•˜ì¸ ì›ì†Œ ìœ„ì¹˜ ì €ì¥
        out = x.copy()
        out[self.mask] = 0
        return out
    
    def backward(self, dout):
        dout[self.mask] = 0  # 0 ì´í•˜ì˜€ë˜ ê³³ì€ ê¸°ìš¸ê¸° 0
        return dout
```

#### 4. Sigmoid ê³„ì¸µ
```python
class Sigmoid:
    def __init__(self):
        self.out = None
    
    def forward(self, x):
        out = 1 / (1 + np.exp(-x))
        self.out = out
        return out
    
    def backward(self, dout):
        dx = dout * self.out * (1.0 - self.out)
        return dx
```

**ì‹œê·¸ëª¨ì´ë“œ ë¯¸ë¶„ ê³µì‹**:
$$\sigma'(x) = \sigma(x)(1 - \sigma(x))$$

#### 5. Affine ê³„ì¸µ (ì™„ì „ì—°ê²°ì¸µ)
```python
class Affine:
    def __init__(self, W, b):
        self.W = W
        self.b = b
        self.x = None
        self.dW = None
        self.db = None
    
    def forward(self, x):
        self.x = x
        out = np.dot(x, self.W) + self.b
        return out
    
    def backward(self, dout):
        dx = np.dot(dout, self.W.T)
        self.dW = np.dot(self.x.T, dout)
        self.db = np.sum(dout, axis=0)
        return dx
```

**í–‰ë ¬ ë¯¸ë¶„ ê·œì¹™**:
- $Y = XW + B$ì¼ ë•Œ
- $\frac{\partial L}{\partial X} = \frac{\partial L}{\partial Y} \cdot W^T$
- $\frac{\partial L}{\partial W} = X^T \cdot \frac{\partial L}{\partial Y}$

#### 6. Softmax-with-Loss ê³„ì¸µ
```python
class SoftmaxWithLoss:
    def __init__(self):
        self.loss = None
        self.y = None  # softmax ì¶œë ¥
        self.t = None  # ì •ë‹µ ë ˆì´ë¸”
    
    def forward(self, x, t):
        self.t = t
        self.y = softmax(x)
        self.loss = cross_entropy_error(self.y, self.t)
        return self.loss
    
    def backward(self, dout=1):
        batch_size = self.t.shape[0]
        dx = (self.y - self.t) / batch_size
        return dx
```

> [!tip] ì¤‘ìš”í•œ ì„±ì§ˆ
> Softmax + Cross Entropyì˜ ì—­ì „íŒŒëŠ” ë§¤ìš° ê°„ë‹¨!
> $$\frac{\partial L}{\partial a} = y - t$$
> - $y$: softmax ì¶œë ¥
> - $t$: ì •ë‹µ ë ˆì´ë¸” (ì›-í•«)

---

## í•µì‹¬ ìˆ˜ì‹ ëª¨ìŒ

### í™œì„±í™” í•¨ìˆ˜

| í•¨ìˆ˜ | ìˆ˜ì‹ | ë¯¸ë¶„ |
|------|------|------|
| Sigmoid | $\sigma(x) = \frac{1}{1+e^{-x}}$ | $\sigma'(x) = \sigma(x)(1-\sigma(x))$ |
| ReLU | $\max(0, x)$ | $\begin{cases} 1 & (x>0) \\ 0 & (x\leq0) \end{cases}$ |
| Softmax | $y_k = \frac{e^{a_k}}{\sum_i e^{a_i}}$ | - |

### ì†ì‹¤ í•¨ìˆ˜

| í•¨ìˆ˜ | ìˆ˜ì‹ | ìš©ë„ |
|------|------|------|
| MSE | $E = \frac{1}{2}\sum(y-t)^2$ | íšŒê·€ |
| CEE | $E = -\sum t \log y$ | ë¶„ë¥˜ |

### ê²½ì‚¬ í•˜ê°•ë²•

$$W \leftarrow W - \eta \frac{\partial L}{\partial W}$$

### ì˜¤ì°¨ì—­ì „íŒŒ í•µì‹¬

$$\frac{\partial L}{\partial x} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial x}$$

---

## ì‹œí—˜ ì¶œì œ ì˜ˆìƒ ë¬¸ì œ

### 1. ê°œë… ë¬¸ì œ

> [!question] Q1. í¼ì…‰íŠ¸ë¡ ê³¼ ì‹ ê²½ë§ì˜ ì°¨ì´ì ì€?
> **ë‹µ**: 
> - í¼ì…‰íŠ¸ë¡ : ë‹¨ì¸µ êµ¬ì¡°, ì„ í˜• ë¶„ë¦¬ ê°€ëŠ¥í•œ ë¬¸ì œë§Œ í•´ê²°
> - ì‹ ê²½ë§: ë‹¤ì¸µ êµ¬ì¡°, ë¹„ì„ í˜• í™œì„±í™” í•¨ìˆ˜ ì‚¬ìš©, ë³µì¡í•œ ë¬¸ì œ í•´ê²° ê°€ëŠ¥

> [!question] Q2. ì™œ í™œì„±í™” í•¨ìˆ˜ëŠ” ë¹„ì„ í˜• í•¨ìˆ˜ì—¬ì•¼ í•˜ëŠ”ê°€?
> **ë‹µ**: ì„ í˜• í•¨ìˆ˜ë§Œ ì‚¬ìš©í•˜ë©´ ì¸µì„ ì•„ë¬´ë¦¬ ê¹Šê²Œ í•´ë„ í•˜ë‚˜ì˜ ì„ í˜• í•¨ìˆ˜ë¡œ í‘œí˜„ ê°€ëŠ¥. ë¹„ì„ í˜• í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ì•¼ ì¸µì„ ìŒ“ëŠ” ì˜ë¯¸ê°€ ìˆìŒ.

> [!question] Q3. í•™ìŠµ ì‹œ ì •í™•ë„ê°€ ì•„ë‹Œ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ëŠ” ì´ìœ ëŠ”?
> **ë‹µ**: ì •í™•ë„ëŠ” ëŒ€ë¶€ë¶„ì˜ ì¥ì†Œì—ì„œ ë¯¸ë¶„ê°’ì´ 0ì´ ë˜ì–´ ë§¤ê°œë³€ìˆ˜ë¥¼ ê°±ì‹ í•  ìˆ˜ ì—†ìŒ. ì†ì‹¤ í•¨ìˆ˜ëŠ” ì—°ì†ì ì´ê³  ë¯¸ë¶„ ê°€ëŠ¥í•˜ì—¬ ê²½ì‚¬í•˜ê°•ë²• ì ìš© ê°€ëŠ¥.

### 2. ê³„ì‚° ë¬¸ì œ

> [!question] Q4. AND ê²Œì´íŠ¸ì˜ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì„ ì„¤ì •í•˜ì‹œì˜¤.
> **ë‹µ**: 
> ```python
> w1 = 0.5, w2 = 0.5, b = -0.7
> # ë˜ëŠ” ë‹¤ë¥¸ ê°’ë„ ê°€ëŠ¥ (ì¡°ê±´: w1*1 + w2*1 + b > 0)
> ```

> [!question] Q5. ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ Ïƒ(x) = 1/(1+e^(-x))ì˜ ë¯¸ë¶„ì„ êµ¬í•˜ì‹œì˜¤.
> **ë‹µ**: Ïƒ'(x) = Ïƒ(x)(1 - Ïƒ(x))

### 3. êµ¬í˜„ ë¬¸ì œ

> [!question] Q6. ReLU í•¨ìˆ˜ë¥¼ NumPyë¡œ êµ¬í˜„í•˜ì‹œì˜¤.
> **ë‹µ**:
> ```python
> def relu(x):
>     return np.maximum(0, x)
> ```

> [!question] Q7. êµì°¨ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨ í•¨ìˆ˜ë¥¼ êµ¬í˜„í•˜ì‹œì˜¤.
> **ë‹µ**:
> ```python
> def cross_entropy_error(y, t):
>     delta = 1e-7
>     return -np.sum(t * np.log(y + delta))
> ```

### 4. ì˜¤ì°¨ì—­ì „íŒŒ ë¬¸ì œ

> [!question] Q8. ê³±ì…ˆ ê³„ì¸µì˜ ì—­ì „íŒŒë¥¼ ì„¤ëª…í•˜ì‹œì˜¤.
> **ë‹µ**:
> - ìˆœì „íŒŒ: z = x * y
> - ì—­ì „íŒŒ: 
>   - âˆ‚z/âˆ‚x = y â†’ dx = dout * y
>   - âˆ‚z/âˆ‚y = x â†’ dy = dout * x

> [!question] Q9. Affine ê³„ì¸µì—ì„œ ê°€ì¤‘ì¹˜ Wì˜ ê¸°ìš¸ê¸°ë¥¼ êµ¬í•˜ëŠ” ì‹ì€?
> **ë‹µ**: dW = X^T Â· dout

---

## í•µì‹¬ ì•”ê¸° ì‚¬í•­

> [!important] ë°˜ë“œì‹œ ì•”ê¸°!
> 
> ### í™œì„±í™” í•¨ìˆ˜ ì„ íƒ
> - **ì€ë‹‰ì¸µ**: ReLU (í˜„ì¬ í‘œì¤€)
> - **ì¶œë ¥ì¸µ (íšŒê·€)**: í•­ë“± í•¨ìˆ˜
> - **ì¶œë ¥ì¸µ (ì´ì§„ ë¶„ë¥˜)**: Sigmoid
> - **ì¶œë ¥ì¸µ (ë‹¤ì¤‘ ë¶„ë¥˜)**: Softmax
> 
> ### ì†ì‹¤ í•¨ìˆ˜ ì„ íƒ
> - **íšŒê·€ ë¬¸ì œ**: MSE (í‰ê·  ì œê³± ì˜¤ì°¨)
> - **ë¶„ë¥˜ ë¬¸ì œ**: CEE (êµì°¨ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨)
> 
> ### í•™ìŠµ ê³¼ì •
> 1. ë¯¸ë‹ˆë°°ì¹˜: í›ˆë ¨ ë°ì´í„° ì¤‘ ì¼ë¶€ë¥¼ ë¬´ì‘ìœ„ë¡œ ì„ íƒ
> 2. ê¸°ìš¸ê¸° ê³„ì‚°: ì†ì‹¤ í•¨ìˆ˜ì˜ ë¯¸ë¶„ ê³„ì‚°
> 3. ë§¤ê°œë³€ìˆ˜ ê°±ì‹ : W â† W - Î·Â·âˆ‡W
> 4. ë°˜ë³µ
> 
> ### ì˜¤ì°¨ì—­ì „íŒŒë²• ì¥ì 
> - ìˆ˜ì¹˜ ë¯¸ë¶„ë³´ë‹¤ í›¨ì”¬ ë¹ ë¦„
> - ë§¤ê°œë³€ìˆ˜ê°€ ë§ì•„ë„ íš¨ìœ¨ì ìœ¼ë¡œ ê³„ì‚° ê°€ëŠ¥

---

## ì¶”ê°€ í•™ìŠµ ìë£Œ

- [[Ch2. í¼ì…‰íŠ¸ë¡  ìƒì„¸ ì •ë¦¬]]
- [[Ch4. ê²½ì‚¬ í•˜ê°•ë²• ìµœì í™”]]
- [[Ch5. ì˜¤ì°¨ì—­ì „íŒŒë²• ìˆ˜í•™ì  ì¦ëª…]]

---

> [!success] ì‹œí—˜ ê³µë¶€ ì²´í¬ë¦¬ìŠ¤íŠ¸
> - [ ] ê° í™œì„±í™” í•¨ìˆ˜ì˜ íŠ¹ì§•ê³¼ ë¯¸ë¶„ ê³µì‹ ì•”ê¸°
> - [ ] ì†ì‹¤ í•¨ìˆ˜ ê³µì‹ê³¼ ìš©ë„ ì•”ê¸°
> - [ ] í¼ì…‰íŠ¸ë¡ ìœ¼ë¡œ XORë¥¼ êµ¬í˜„í•  ìˆ˜ ì—†ëŠ” ì´ìœ  ì´í•´
> - [ ] ê²½ì‚¬ í•˜ê°•ë²• ìˆ˜ì‹ ì•”ê¸°
> - [ ] ì£¼ìš” ê³„ì¸µì˜ ìˆœì „íŒŒ/ì—­ì „íŒŒ êµ¬í˜„ ì´í•´
> - [ ] Softmax-with-Lossì˜ ì—­ì „íŒŒê°€ y-tì¸ ì´ìœ  ì´í•´
