---
tags: [neural-network, deep-learning, exam, summary]
aliases: [ì‹ ê²½ë§ ì‹œí—˜ ì •ë¦¬, NN Exam Notes]
created: 2025-10-18
---

# ì‹ ê²½ë§ í•µì‹¬ ì´ë¡  ì •ë¦¬ (ì‹œí—˜ ëŒ€ë¹„)

> [!info] ë¬¸ì„œ ì •ë³´
> - ê³¼ëª©: ì‹ ê²½ë§ (Neural Network)
> - ëª©ì : ì‹œí—˜ ëŒ€ë¹„ í•µì‹¬ ê°œë… ì •ë¦¬
> - êµ¬ì„±: íŒŒì´ì¬ ê¸°ì´ˆ â†’ í¼ì…‰íŠ¸ë¡  â†’ ì‹ ê²½ë§ â†’ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ â†’ ì˜¤ì°¨ì—­ì „íŒŒë²•

---

## ğŸ“š ëª©ì°¨

1. [[#Ch1. íŒŒì´ì¬ ê¸°ì´ˆ]]
2. [[#Ch2. í¼ì…‰íŠ¸ë¡  (Perceptron)]]
3. [[#Ch3. ì‹ ê²½ë§ (Neural Network)]]
4. [[#Ch4. í•™ìŠµ ì•Œê³ ë¦¬ì¦˜]]
5. [[#Ch5. ì˜¤ì°¨ì—­ì „íŒŒë²• (Backpropagation)]]
6. [[#í•µì‹¬ ìˆ˜ì‹ ëª¨ìŒ]]
7. [[#ì‹œí—˜ ì¶œì œ ì˜ˆìƒ ë¬¸ì œ]]

---

## Ch1. íŒŒì´ì¬ ê¸°ì´ˆ

### NumPy í•µì‹¬ ê°œë…

#### ë¸Œë¡œë“œìºìŠ¤íŒ… (Broadcasting)
```python
# ìŠ¤ì¹¼ë¼ì™€ ë°°ì—´ ì—°ì‚°
x = np.array([1, 2, 3])
x * 2  # [2, 4, 6] - ìë™ìœ¼ë¡œ ëª¨ë“  ì›ì†Œì— ì ìš©

# ë‹¤ì°¨ì› ë°°ì—´ ë¸Œë¡œë“œìºìŠ¤íŒ…
A = np.array([[1, 2], [3, 4]])
B = np.array([10, 20])
A * B  # [[10, 40], [30, 80]] - Bê°€ ê° í–‰ì— ì ìš©ë¨
```

> [!tip] ì•”ê¸° í¬ì¸íŠ¸
> - **ë¸Œë¡œë“œìºìŠ¤íŒ…**: í˜•ìƒì´ ë‹¤ë¥¸ ë°°ì—´ ê°„ ì—°ì‚° ì‹œ ìë™ìœ¼ë¡œ í˜•ìƒì„ ë§ì¶°ì¤Œ
> - **ì›ì†Œë³„ ì—°ì‚° (element-wise)**: `*`, `+`, `-`, `/` ë“±ì€ ì›ì†Œë³„ë¡œ ê³„ì‚°
> - **í–‰ë ¬ ê³±**: `np.dot(A, B)` ì‚¬ìš© (ë‚´ì )

#### NumPy ë°°ì—´ ì—°ì‚° ì •ë¦¬

| ì—°ì‚° | ì½”ë“œ | ì„¤ëª… |
|------|------|------|
| ì›ì†Œë³„ ê³± | `A * B` | element-wise product |
| í–‰ë ¬ ê³± | `np.dot(A, B)` | matrix multiplication |
| ì „ì¹˜ í–‰ë ¬ | `A.T` ë˜ëŠ” `np.transpose(A)` | transpose |
| í‰íƒ„í™” | `A.flatten()` | 1ì°¨ì› ë°°ì—´ë¡œ ë³€í™˜ |

---

## Ch2. í¼ì…‰íŠ¸ë¡  (Perceptron)

### í¼ì…‰íŠ¸ë¡ ì˜ ì •ì˜

> [!important] í•µì‹¬ ê°œë…
> **í¼ì…‰íŠ¸ë¡ **: ë‹¤ìˆ˜ì˜ ì‹ í˜¸ë¥¼ ì…ë ¥ë°›ì•„ í•˜ë‚˜ì˜ ì‹ í˜¸ë¥¼ ì¶œë ¥í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜
> - ì…ë ¥: $x_1, x_2, ..., x_n$
> - ê°€ì¤‘ì¹˜: $w_1, w_2, ..., w_n$
> - í¸í–¥: $b$
> - ì¶œë ¥: $y = \begin{cases} 0 & (w_1x_1 + w_2x_2 + b \leq 0) \\ 1 & (w_1x_1 + w_2x_2 + b > 0) \end{cases}$

### ê°€ì¤‘ì¹˜ì™€ í¸í–¥

- **ê°€ì¤‘ì¹˜ (weight, w)**: ê° ì…ë ¥ ì‹ í˜¸ì˜ ì¤‘ìš”ë„ë¥¼ ë‚˜íƒ€ëƒ„
  - ê°€ì¤‘ì¹˜ê°€ í´ìˆ˜ë¡ í•´ë‹¹ ì…ë ¥ì´ ì¤‘ìš”í•¨
  
- **í¸í–¥ (bias, b)**: ë‰´ëŸ°ì´ ì–¼ë§ˆë‚˜ ì‰½ê²Œ í™œì„±í™”ë˜ëŠ”ì§€ ì¡°ì •
  - í¸í–¥ì´ í´ìˆ˜ë¡ ì‰½ê²Œ í™œì„±í™”ë¨
  - í¸í–¥ì´ ì‘ì„ìˆ˜ë¡ í™œì„±í™” ì–´ë ¤ì›€

### ë…¼ë¦¬ ê²Œì´íŠ¸ êµ¬í˜„

#### AND ê²Œì´íŠ¸
```python
def AND(x1, x2):
    w1, w2, b = 0.5, 0.5, -0.7
    tmp = w1*x1 + w2*x2 + b
    return 1 if tmp > 0 else 0
```

| x1 | x2 | y |
|----|----|---|
| 0  | 0  | 0 |
| 0  | 1  | 0 |
| 1  | 0  | 0 |
| 1  | 1  | 1 |

#### OR ê²Œì´íŠ¸
```python
def OR(x1, x2):
    w1, w2, b = 0.5, 0.5, -0.2  # í¸í–¥ì´ ANDë³´ë‹¤ í¼
    tmp = w1*x1 + w2*x2 + b
    return 1 if tmp > 0 else 0
```

#### NAND ê²Œì´íŠ¸ (NOT AND)
```python
def NAND(x1, x2):
    w1, w2, b = -0.5, -0.5, 0.7  # ê°€ì¤‘ì¹˜ê°€ ìŒìˆ˜
    tmp = w1*x1 + w2*x2 + b
    return 1 if tmp > 0 else 0
```

### ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡  (Multi-Layer Perceptron)

> [!warning] ì¤‘ìš” ê°œë…
> **XOR ê²Œì´íŠ¸ëŠ” ë‹¨ì¸µ í¼ì…‰íŠ¸ë¡ ìœ¼ë¡œ êµ¬í˜„ ë¶ˆê°€ëŠ¥**
> - ì´ìœ : ì„ í˜• ë¶„ë¦¬ ë¶ˆê°€ëŠ¥ (ë¹„ì„ í˜• ë¬¸ì œ)
> - í•´ê²°: ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡  í•„ìš”

#### XOR ê²Œì´íŠ¸ (2ì¸µ êµ¬ì¡°)
```python
def XOR(x1, x2):
    s1 = NAND(x1, x2)  # ì€ë‹‰ì¸µ 1
    s2 = OR(x1, x2)    # ì€ë‹‰ì¸µ 2
    y = AND(s1, s2)    # ì¶œë ¥ì¸µ
    return y
```

**XOR ì§„ë¦¬í‘œ**:

| x1 | x2 | NAND | OR | AND (ì¶œë ¥) |
|----|----|------|----|----|
| 0  | 0  | 1    | 0  | 0  |
| 0  | 1  | 1    | 1  | 1  |
| 1  | 0  | 1    | 1  | 1  |
| 1  | 1  | 0    | 1  | 0  |

---

## Ch3. ì‹ ê²½ë§ (Neural Network)

### í™œì„±í™” í•¨ìˆ˜ (Activation Function)

> [!important] ì™œ í™œì„±í™” í•¨ìˆ˜ê°€ í•„ìš”í•œê°€?
> - ì„ í˜• í•¨ìˆ˜ë§Œ ì‚¬ìš©í•˜ë©´ ì¸µì„ ì•„ë¬´ë¦¬ ê¹Šê²Œ í•´ë„ ë‹¨ì¸µê³¼ ë™ì¼
> - ë¹„ì„ í˜• í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ì•¼ ì¸µì„ ìŒ“ëŠ” ì˜ë¯¸ê°€ ìˆìŒ

#### 1. ê³„ë‹¨ í•¨ìˆ˜ (Step Function)
```python
def step_function(x):
    return np.array(x > 0, dtype=int)
```
- ì„ê³„ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ 0 ë˜ëŠ” 1 ì¶œë ¥
- **ë‹¨ì **: ë¯¸ë¶„ ë¶ˆê°€ëŠ¥ â†’ í•™ìŠµ ë¶ˆê°€

#### 2. ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ (Sigmoid Function)
$$\sigma(x) = \frac{1}{1 + e^{-x}}$$

```python
def sigmoid(x):
    return 1 / (1 + np.exp(-x))
```

**íŠ¹ì§•**:
- ì¶œë ¥ ë²”ìœ„: (0, 1)
- ë§¤ë„ëŸ¬ìš´ ê³¡ì„ 
- ë¯¸ë¶„ ê°€ëŠ¥
- **ë‹¨ì **: ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œ (gradient vanishing)

**ë¯¸ë¶„**:
$$\sigma'(x) = \sigma(x)(1 - \sigma(x))$$

#### 3. ReLU í•¨ìˆ˜ (Rectified Linear Unit)
$$\text{ReLU}(x) = \max(0, x)$$

```python
def relu(x):
    return np.maximum(0, x)
```

**íŠ¹ì§•**:
- $x > 0$: ì¶œë ¥ = x (ê¸°ìš¸ê¸° 1)
- $x \leq 0$: ì¶œë ¥ = 0 (ê¸°ìš¸ê¸° 0)
- **ì¥ì **: 
  - ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œ í•´ê²°
  - ê³„ì‚° íš¨ìœ¨ì 
  - í˜„ì¬ ê°€ì¥ ë§ì´ ì‚¬ìš©ë¨

**ë¯¸ë¶„**:
$$\text{ReLU}'(x) = \begin{cases} 1 & (x > 0) \\ 0 & (x \leq 0) \end{cases}$$

#### 4. í•˜ì´í¼ë³¼ë¦­ íƒ„ì  íŠ¸ (Tanh)
$$\tanh(x)=\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}$$

**íŠ¹ì§•**:
- ì¶œë ¥ ë²”ìœ„ê°€ $(-1, 1)$ë¡œ ì‹œê·¸ëª¨ì´ë“œë³´ë‹¤ ì¤‘ì‹¬ì´ 0ì— ê°€ê¹Œì›Œ ìˆ˜ë ´ì´ ë¹ ë¥¸ í¸
- $\tanh(x)=2\cdot\text{sigmoid}(2x)-1$ ê´€ê³„ ê¸°ì–µí•˜ê¸°

#### 5. Leaky ReLU
$$f(x)=\begin{cases}x & x\ge0 \\ \alpha x & x<0\end{cases}$$

**í¬ì¸íŠ¸**:
- ReLUì˜ $0$ ì´í•˜ êµ¬ê°„ ê¸°ìš¸ê¸° 0 ë¬¸ì œë¥¼ $\alpha(0<\alpha\ll1)$ë¡œ ì™„í™”
- ì‹œí—˜ì—ì„œëŠ” $\alpha$ê°€ ì‘ì€ ìƒìˆ˜ì„ì„ í™•ì¸ì‹œí‚¤ëŠ” ë¬¸ì œê°€ ìì£¼ ë‚˜ì˜´

#### 6. ELU (Exponential Linear Unit)
$$f(x)=\begin{cases}x & x\ge0 \\ \alpha\left(e^{x}-1\right) & x<0\end{cases}$$

**íŠ¹ì§•**:
- $x<0$ êµ¬ê°„ë„ ë¶€ë“œëŸ½ê³  ë¯¸ë¶„ ê°€ëŠ¥
- í‰ê·  ì¶œë ¥ì´ 0ì— ê°€ê¹Œì›Œ í•™ìŠµ ì•ˆì •í™”ì— ë„ì›€

#### 7. Swish
$$\text{swish}(x) = x\cdot \text{sigmoid}(x)$$

**íŠ¹ì§•**:
- $x<0$ ì˜ì—­ì—ì„œë„ ì™„ë§Œí•˜ê²Œ ê°ì†Œ â†’ ê¹Šì€ ë„¤íŠ¸ì›Œí¬ì—ì„œ ì„±ëŠ¥ ìš°ìˆ˜
- êµ¬ê¸€ì—ì„œ ì œì•ˆ, ì‹œí—˜ì—ì„œëŠ” ì‹œê·¸ëª¨ì´ë“œì™€ì˜ ê³± êµ¬ì¡°ë¥¼ ë¬»ëŠ” ë‹¨ë‹µí˜• ê°€ëŠ¥

#### 8. GELU (Gaussian Error Linear Unit)
$$\text{GELU}(x)=x\cdot \Phi(x)$$

**í¬ì¸íŠ¸**:
- $\Phi(x)$: í‘œì¤€ ì •ê·œë¶„í¬ì˜ ëˆ„ì ë¶„í¬í•¨ìˆ˜(CDF)
- ìµœê·¼ Transformer ê³„ì—´ì—ì„œ ê¸°ë³¸ í™œì„±í™” í•¨ìˆ˜ â†’ í˜„ëŒ€ ì‹ ê²½ë§ íŠ¸ë Œë“œ ëŒ€ë¹„ ë¬¸ì œ

### ì¶œë ¥ì¸µ í™œì„±í™” í•¨ìˆ˜

#### ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ (Softmax)
$$y_k = \frac{\exp(a_k)}{\sum_{i=1}^{n} \exp(a_i)}$$

```python
def softmax(x):
    c = np.max(x)  # ì˜¤ë²„í”Œë¡œ ë°©ì§€
    exp_x = np.exp(x - c)
    return exp_x / np.sum(exp_x)
```

**íŠ¹ì§•**:
- ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ì— ì‚¬ìš©
- ì¶œë ¥ì˜ í•©ì´ 1 (í™•ë¥  ë¶„í¬)
- ì˜¤ë²„í”Œë¡œ ë°©ì§€ë¥¼ ìœ„í•´ ìµœëŒ“ê°’ì„ ë¹¼ì¤Œ

> [!tip] ì¶œë ¥ì¸µ í™œì„±í™” í•¨ìˆ˜ ì„ íƒ
> - **íšŒê·€ ë¬¸ì œ**: í•­ë“± í•¨ìˆ˜ (identity function)
> - **ì´ì§„ ë¶„ë¥˜**: ì‹œê·¸ëª¨ì´ë“œ
> - **ë‹¤ì¤‘ ë¶„ë¥˜**: ì†Œí”„íŠ¸ë§¥ìŠ¤

> [!note] Softmax í•µì‹¬ ì„±ì§ˆ
> - ëª¨ë“  ì¶œë ¥ì´ 0 ì´ìƒì´ë©° í•©ì´ 1 â†’ í™•ë¥  ë²¡í„° í˜•ì„±
> - $ \text{softmax}(\mathbf{a}+c)=\text{softmax}(\mathbf{a}) $ (ëª¨ë“  ì„±ë¶„ì— ë™ì¼ ìƒìˆ˜ $c$ ì¶”ê°€ ì‹œ ê²°ê³¼ ë™ì¼)
> - ì‹¤ì „ ê³„ì‚° ì‹œ $\max(\mathbf{a})$ë¥¼ ë¹¼ì„œ ìˆ˜ì¹˜ì  ì•ˆì •ì„± í™•ë³´

### 3ì¸µ ì‹ ê²½ë§ êµ¬ì¡° & í–‰ë ¬ í‘œí˜„

- ì…ë ¥ $\mathbf{x}$ â†’ ì€ë‹‰ì¸µ 1 â†’ ì€ë‹‰ì¸µ 2 â†’ ì¶œë ¥ì¸µìœ¼ë¡œ **Affine â†’ í™œì„±í™”** ì¡°í•© ë°˜ë³µ
- ê° ì¸µ ìˆœì „íŒŒ:
  - 1ì¸µ: $\mathbf{z}^{(1)} = W^{(1)}\mathbf{x} + \mathbf{b}^{(1)}$, $\mathbf{a}^{(1)} = h(\mathbf{z}^{(1)})$
  - 2ì¸µ: $\mathbf{z}^{(2)} = W^{(2)}\mathbf{a}^{(1)} + \mathbf{b}^{(2)}$, $\mathbf{a}^{(2)} = h(\mathbf{z}^{(2)})$
  - ì¶œë ¥ì¸µ: $\mathbf{l} = W^{(3)}\mathbf{a}^{(2)} + \mathbf{b}^{(3)}$, $\mathbf{y} = \text{softmax}(\mathbf{l})$
- **í–‰ë ¬ ì°¨ì› ì‹œí—˜ í¬ì¸íŠ¸**:
  - $W^{(l)}$: (í•´ë‹¹ ì¸µ ë‰´ëŸ° ìˆ˜ Ã— ì´ì „ ì¸µ ë‰´ëŸ° ìˆ˜)
  - $\mathbf{b}^{(l)}$: (í•´ë‹¹ ì¸µ ë‰´ëŸ° ìˆ˜ Ã— 1)
  - í–‰ë ¬ ê³±ì´ ê°€ëŠ¥í•˜ë ¤ë©´ ì—´Â·í–‰ í¬ê¸° ì¼ì¹˜ í™•ì¸ í•„ìˆ˜
- í”íˆ ì‚¬ìš©í•˜ëŠ” êµ¬ì„±: `Affine â†’ Sigmoid â†’ Affine â†’ Sigmoid â†’ Affine â†’ Softmax`

### MNIST ë°ì´í„°ì…‹ í•µì‹¬ ìŠ¤í™

- ìˆ«ì 0~9 ì†ê¸€ì”¨ ì´ë¯¸ì§€, **í›ˆë ¨ 60,000ì¥ / í…ŒìŠ¤íŠ¸ 10,000ì¥**
- ê° ì´ë¯¸ì§€ëŠ” $28 \times 28$ í”½ì…€, í•œ í”½ì…€ ë°ê¸°ëŠ” 0~255
- ì…ë ¥ ë²¡í„°ëŠ” 784ì°¨ì›(=28Ã—28)ìœ¼ë¡œ í‰íƒ„í™”í•˜ì—¬ ì‚¬ìš©
- ë¼ë²¨ì€ 10ì°¨ì› ì›-í•« ì¸ì½”ë”©ìœ¼ë¡œ í‘œí˜„

> [!important] ì‹œí—˜ ëŒ€ë¹„: ë°ì´í„° í¬ê¸°Â·ë¶„í• Â·í•´ìƒë„ëŠ” ë¹ˆì¶œ ì•”ê¸° ìš”ì†Œ

---

## Ch4. í•™ìŠµ ì•Œê³ ë¦¬ì¦˜

### ë¼ë²¨ ì „ì²˜ë¦¬ (One-hot Encoding)

- ë¶„ë¥˜ ë¬¸ì œì—ì„œëŠ” ì •ë‹µ ë ˆì´ë¸”ì„ ì›-í•« ë²¡í„°ë¡œ ë³€í™˜
  - ì˜ˆ: ìˆ«ì 3 â†’ `[0,0,0,1,0,0,0,0,0,0]`
  - ì˜ˆ: ìˆ«ì 7 â†’ `[0,0,0,0,0,0,0,1,0,0]`
- ì†Œí”„íŠ¸ë§¥ìŠ¤ ì¶œë ¥ê³¼ ë™ì¼ ê¸¸ì´ë¡œ ë§ì¶° ì†ì‹¤ í•¨ìˆ˜ë¥¼ ê³„ì‚° ê°€ëŠ¥í•˜ê²Œ í•¨

> [!important] ì‹œí—˜ ì²´í¬
> - ê° ìë¦¿ìˆ˜ì— ëŒ€ì‘í•˜ëŠ” í´ë˜ìŠ¤ ìœ„ì¹˜ë¥¼ ì •í™•íˆ ë§¤í•‘í•´ì•¼ í•¨
> - ë¼ë²¨ ë²¡í„°ëŠ” í™•ë¥  ë¶„í¬ê°€ ì•„ë‹ˆì§€ë§Œ, ì •ë‹µ í´ë˜ìŠ¤ì—ë§Œ 1ì„ ë‘ëŠ” êµ¬ì¡°ë¼ëŠ” ì ì„ ë¬»ëŠ” ë¬¸ì œê°€ ì¦ìŒ

### ì†ì‹¤ í•¨ìˆ˜ (Loss Function)

> [!important] ì†ì‹¤ í•¨ìˆ˜ì˜ ì—­í• 
> - ì‹ ê²½ë§ì˜ ì„±ëŠ¥ì„ "ë‚˜ì¨ì˜ ì •ë„"ë¡œ ì¸¡ì •
> - í•™ìŠµì˜ ëª©í‘œ: ì†ì‹¤ í•¨ìˆ˜ë¥¼ ìµœì†Œí™”í•˜ëŠ” ê°€ì¤‘ì¹˜ ì°¾ê¸°
> - ì •í™•ë„ê°€ ì•„ë‹Œ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ëŠ” ì´ìœ : ë¯¸ë¶„ ê°€ëŠ¥í•´ì•¼ í•¨

#### 1. í‰ê·  ì œê³± ì˜¤ì°¨ (MSE, Mean Squared Error)
$$E = \frac{1}{2}\sum_{k}(y_k - t_k)^2$$

```python
def mean_squared_error(y, t):
    return 0.5 * np.sum((y - t)**2)
```

- ì£¼ë¡œ **íšŒê·€ ë¬¸ì œ**ì— ì‚¬ìš©
- $y$: ì‹ ê²½ë§ ì¶œë ¥, $t$: ì •ë‹µ ë ˆì´ë¸”

#### 2. êµì°¨ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨ (CEE, Cross Entropy Error)
$$E = -\sum_{k} t_k \log y_k$$

```python
def cross_entropy_error(y, t):
    delta = 1e-7  # log(0) ë°©ì§€
    return -np.sum(t * np.log(y + delta))
```

- ì£¼ë¡œ **ë¶„ë¥˜ ë¬¸ì œ**ì— ì‚¬ìš©
- $t$: ì›-í•« ì¸ì½”ë”©ëœ ì •ë‹µ ë ˆì´ë¸”
- $y$: ì†Œí”„íŠ¸ë§¥ìŠ¤ ì¶œë ¥ (í™•ë¥ )

> [!warning] ì‹œí—˜ ì¶œì œ í¬ì¸íŠ¸
> - MSEëŠ” íšŒê·€, CEEëŠ” ë¶„ë¥˜
> - CEEì—ì„œ `delta`ë¥¼ ë”í•˜ëŠ” ì´ìœ : log(0) = -âˆ ë°©ì§€
> - ì›-í•« ì¸ì½”ë”©: [0, 0, 1, 0, 0] í˜•íƒœ

> [!note] ë°ì´í„°ì…‹ ì†ì‹¤ ì‚°ì¶œ
> - ë‹¨ì¼ ìƒ˜í”Œ ì†ì‹¤ì„ êµ¬í•œ ë’¤ **ì „ì²´ ë°ì´í„° í‰ê· **ìœ¼ë¡œ J(w) ê³„ì‚°
> - $E_{\text{dataset}} = \frac{1}{N}\sum_{n=1}^{N}E^{(n)}$ í˜•íƒœë¼ëŠ” ì ì„ ê¸°ì–µ

### ë¯¸ë‹ˆë°°ì¹˜ í•™ìŠµ

```python
train_size = x_train.shape[0]
batch_size = 100
batch_mask = np.random.choice(train_size, batch_size)
x_batch = x_train[batch_mask]
t_batch = t_train[batch_mask]
```

- ì „ì²´ ë°ì´í„°ê°€ ì•„ë‹Œ ì¼ë¶€(ë¯¸ë‹ˆë°°ì¹˜)ë§Œ ì‚¬ìš©
- **ì¥ì **: ë©”ëª¨ë¦¬ íš¨ìœ¨ì , í•™ìŠµ ì†ë„ í–¥ìƒ

### ê²½ì‚¬ í•˜ê°•ë²• (Gradient Descent)

$$W \leftarrow W - \eta \frac{\partial L}{\partial W}$$

- $W$: ê°€ì¤‘ì¹˜
- $\eta$: í•™ìŠµë¥  (learning rate)
- $\frac{\partial L}{\partial W}$: ì†ì‹¤ í•¨ìˆ˜ì˜ ê¸°ìš¸ê¸°

```python
def gradient_descent(f, init_x, lr=0.01, step_num=100):
    x = init_x
    for i in range(step_num):
        grad = numerical_gradient(f, x)
        x -= lr * grad  # ê°€ì¤‘ì¹˜ ê°±ì‹ 
    return x
```

- ì¼ë°˜ì‹: $\mathbf{w}_{k+1} = \mathbf{w}_{k} - \eta \nabla J(\mathbf{w}_{k})$
- ê¸°ìš¸ê¸°ëŠ” ì†ì‹¤ì´ ê°€ì¥ í¬ê²Œ ì¦ê°€í•˜ëŠ” ë°©í–¥ì´ë¯€ë¡œ **ë°˜ëŒ€ ë°©í–¥ìœ¼ë¡œ ì´ë™**í•´ì•¼ ìµœì†Œê°’ì— ì ‘ê·¼

> [!important] í•™ìŠµë¥  ì„¤ì •
> - **ë„ˆë¬´ í¬ë©´**: ë°œì‚° (í•™ìŠµ ë¶ˆì•ˆì •)
> - **ë„ˆë¬´ ì‘ìœ¼ë©´**: í•™ìŠµ ë§¤ìš° ëŠë¦¼
> - **ì ì ˆí•œ ê°’**: 0.01 ~ 0.001 ì •ë„

> [!warning] ì´ˆê¸°ê°’ & ìµœì†Œì 
> - ì˜ëª»ëœ ì´ˆê¸° ê°€ì¤‘ì¹˜ëŠ” **local minima** ë˜ëŠ” **saddle point**ì— ë¹ ì§ˆ ìˆ˜ ìˆìŒ
> - ì‹¤ì „ì—ì„œëŠ” ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” ì „ëµ(He, Xavier ë“±)ê³¼ í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ì„ í•¨ê»˜ ì„¤ê³„

### í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²• (SGD)

```python
for epoch in range(num_epochs):
    np.random.shuffle(mini_batches)
    for x_batch, t_batch in mini_batches:
        grad = gradient(x_batch, t_batch)
        W -= lr * grad
```

- ì „ì²´ ë°ì´í„° ëŒ€ì‹  **ë¬´ì‘ìœ„ ë¯¸ë‹ˆë°°ì¹˜**ë¥¼ ì‚¬ìš©í•´ ë§¤ ì—…ë°ì´íŠ¸ ìˆ˜í–‰
- ì¥ì : ê³„ì‚°ëŸ‰ ê°ì†Œ, ì§€ì—­ ìµœì†Œì  íƒˆì¶œ ê°€ëŠ¥ì„± ì¦ê°€, ì˜¨ë¼ì¸ í•™ìŠµ ì í•©
- ìš©ì–´ ë§¤ì¹­: í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²• = **Stochastic Gradient Descent**

### ë³€ìˆ˜ ê°œìˆ˜ ê³„ì‚° ì˜ˆì‹œ

- 2ì¸µ ì‹ ê²½ë§ (784 â†’ 50 â†’ 10) ë§¤ê°œë³€ìˆ˜ ìˆ˜:
  - ê°€ì¤‘ì¹˜: $784 \times 50 + 50 \times 10$
  - í¸í–¥: $50 + 10$
  - **ì´ 39,760ê°œ**
- > [!tip] ì‹œí—˜ì—ì„œ ìì£¼ ìš”êµ¬ë˜ëŠ” ë¹ ë¥¸ ê³„ì‚° ì˜ˆì‹œ â†’ ì¸µë³„ ë‰´ëŸ° ìˆ˜ ê³±ì— í¸í–¥ì„ ë”í•˜ë©´ ë¨

### ìˆ˜ì¹˜ ë¯¸ë¶„ vs í•´ì„ì  ë¯¸ë¶„

#### ìˆ˜ì¹˜ ë¯¸ë¶„ (Numerical Differentiation)
$$\frac{df(x)}{dx} \approx \frac{f(x+h) - f(x-h)}{2h}$$

```python
def numerical_gradient(f, x):
    h = 1e-4  # 0.0001
    grad = np.zeros_like(x)
    
    for idx in range(x.size):
        tmp_val = x[idx]
        # f(x+h)
        x[idx] = tmp_val + h
        fxh1 = f(x)
        # f(x-h)
        x[idx] = tmp_val - h
        fxh2 = f(x)
        # ì¤‘ì‹¬ ì°¨ë¶„ë²•
        grad[idx] = (fxh1 - fxh2) / (2*h)
        x[idx] = tmp_val
    
    return grad
```

- **ì¥ì **: êµ¬í˜„ ê°„ë‹¨, ì •í™•
- **ë‹¨ì **: ê³„ì‚° ëŠë¦¼
- **ìš©ë„**: ì˜¤ì°¨ì—­ì „íŒŒë²• ê²€ì¦ (gradient check)

> [!danger] ê³„ì‚° ë¹„ìš© í­ë°œ
> - ë§¤ê°œë³€ìˆ˜ 39,760ê°œ Ã— 10,000ë²ˆ ë°˜ë³µ í•™ìŠµì´ë¼ë©´ **ìˆ˜ì¹˜ ë¯¸ë¶„ë§Œ**ìœ¼ë¡œëŠ” ê³„ì‚° ë¶ˆê°€ ìˆ˜ì¤€
> - í•©ì„± í•¨ìˆ˜(affine, sigmoid, softmax, CEE)ëŠ” **chain rule ê¸°ë°˜ í•´ì„ì  ë¯¸ë¶„(=ì˜¤ì°¨ì—­ì „íŒŒ)**ìœ¼ë¡œ ì²˜ë¦¬í•´ì•¼ ì‹¤ì‹œê°„ í•™ìŠµ ê°€ëŠ¥

---

## Ch5. ì˜¤ì°¨ì—­ì „íŒŒë²• (Backpropagation)

### ê³„ì‚° ê·¸ë˜í”„

> [!important] ê³„ì‚° ê·¸ë˜í”„ ê°œë…
> - ê³„ì‚° ê³¼ì •ì„ ê·¸ë˜í”„ë¡œ í‘œí˜„
> - **ìˆœì „íŒŒ (Forward)**: ì…ë ¥ â†’ ì¶œë ¥ ê³„ì‚°
> - **ì—­ì „íŒŒ (Backward)**: ë¯¸ë¶„(ê¸°ìš¸ê¸°) ê³„ì‚°

### ì—°ì‡„ ë²•ì¹™ (Chain Rule)

$$\frac{\partial z}{\partial x} = \frac{\partial z}{\partial y} \cdot \frac{\partial y}{\partial x}$$

- í•©ì„± í•¨ìˆ˜ì˜ ë¯¸ë¶„ì€ ê° í•¨ìˆ˜ ë¯¸ë¶„ì˜ ê³±

> [!important] Chain Rule ì•”ê¸° í¬ì¸íŠ¸
> - **ìˆœì „íŒŒ**: ë…¸ë“œ ì¶œë ¥ì„ ì°¨ë¡€ì°¨ë¡€ ê³„ì‚° (ì˜ˆ: ê°€ê²© Ã— ìˆ˜ëŸ‰ â†’ ì„¸ê¸ˆ ê³±)
> - **ì—­ì „íŒŒ**: ì¶œë ¥ ê¸°ìš¸ê¸°ë¥¼ ì…ë ¥ ê¸°ìš¸ê¸°ë¡œ ë˜ëŒë ¤ë³´ë‚´ë©° *ì§€ì—­ ë¯¸ë¶„*ì„ ê³±í•¨
> - í•˜ë‚˜ì˜ ë…¸ë“œì— ì—¬ëŸ¬ ê²½ë¡œê°€ ëª¨ì´ë©´ ê¸°ìš¸ê¸°ë¥¼ **í•©ì‚°**í•˜ê³ , ë‚˜ë‰˜ë©´ ê° ê²½ë¡œë¡œ **ë³µì‚¬**ë¨

### ê¸°ë³¸ ë…¸ë“œ ì—­ì „íŒŒ ê·œì¹™ (ë¹ˆì¶œ)

| ë…¸ë“œ | ìˆœì „íŒŒ | ì—­ì „íŒŒ |
|------|--------|--------|
| ë§ì…ˆ | $z = x + y$ | $\frac{\partial L}{\partial x} = \frac{\partial L}{\partial z}$<br>$\frac{\partial L}{\partial y} = \frac{\partial L}{\partial z}$ |
| ê³±ì…ˆ | $z = xy$ | $\frac{\partial L}{\partial x} = y \cdot \frac{\partial L}{\partial z}$<br>$\frac{\partial L}{\partial y} = x \cdot \frac{\partial L}{\partial z}$ |
| ì—­ìˆ˜ | $z = 1/x$ | $\frac{\partial L}{\partial x} = -x^{-2} \cdot \frac{\partial L}{\partial z}$ |
| ê±°ë“­ì œê³± | $z = x^2$ | $\frac{\partial L}{\partial x} = 2x \cdot \frac{\partial L}{\partial z}$ |
| exp | $z = e^{x}$ | $\frac{\partial L}{\partial x} = e^{x} \cdot \frac{\partial L}{\partial z}$ |
| log | $z = \log x$ | $\frac{\partial L}{\partial x} = \frac{1}{x} \cdot \frac{\partial L}{\partial z}$ |

> [!tip] ì‹œí—˜ íŒ  
> - **ê³±ì…ˆ ë…¸ë“œ**: ë°˜ëŒ€í¸ ê°’ì„ ê³±í•œë‹¤  
> - **ë§ì…ˆ ë…¸ë“œ**: ê·¸ëŒ€ë¡œ ì „ë‹¬í•œë‹¤  
> - ì—­ìˆ˜/ë¡œê·¸/ì§€ìˆ˜ ë…¸ë“œëŠ” ë¯¸ë¶„ ê³µì‹ ê·¸ëŒ€ë¡œ ì™¸ì›Œë‘ê¸°

### ê³„ì‚° ê·¸ë˜í”„ ì˜ˆì‹œ (ì‚¬ê³¼Â·ê·¤ ì¥ë°”êµ¬ë‹ˆ)

- ìˆœì „íŒŒ: `(ì‚¬ê³¼ê°€ê²© Ã— ì‚¬ê³¼ìˆ˜ + ê·¤ê°€ê²© Ã— ê·¤ìˆ˜) Ã— (1 + ì†Œë¹„ì„¸)` êµ¬ì¡°
- ì—­ì „íŒŒ íë¦„:
  1. ìµœì¢… ê¸ˆì•¡ì— ëŒ€í•œ ê¸°ìš¸ê¸° 1ì—ì„œ ì‹œì‘
  2. ê³±ì…ˆ ë…¸ë“œ â†’ ì„¸ê¸ˆ í•­ $1+\text{ì„¸ìœ¨}$ì„ ê³±í•´ ê° í•­ëª©ìœ¼ë¡œ ì „ë‹¬
  3. ë§ì…ˆ ë…¸ë“œ â†’ ì‚¬ê³¼ ë¹„ìš©ê³¼ ê·¤ ë¹„ìš© ë°©í–¥ìœ¼ë¡œ ë™ì¼ ê¸°ìš¸ê¸° ë³µì‚¬
  4. ê° ê³±ì…ˆ ë…¸ë“œ â†’ ìˆ˜ëŸ‰/ê°€ê²© ê°’ì„ ê³±í•´ ìƒëŒ€ ë…¸ë“œì— ì „ë‹¬
- > [!important] ê³„ì‚° ê·¸ë˜í”„ ì‚¬ìš© ëª©ì : ë³µì¡í•œ í•©ì„± í•¨ìˆ˜ë¥¼ **ë…¸ë“œ ë‹¨ìœ„ë¡œ ìª¼ê°œ** ì—­ì „íŒŒë¥¼ ì²´ê³„ì ìœ¼ë¡œ ìˆ˜í–‰

### ì£¼ìš” ê³„ì¸µ (Layer) êµ¬í˜„

#### 1. ê³±ì…ˆ ê³„ì¸µ (Mul Layer)
```python
class MulLayer:
    def __init__(self):
        self.x = None
        self.y = None
    
    def forward(self, x, y):
        self.x = x
        self.y = y
        return x * y
    
    def backward(self, dout):
        dx = dout * self.y  # ìˆœì „íŒŒ ì‹œ yë¥¼ ê³±í–ˆìœ¼ë¯€ë¡œ
        dy = dout * self.x  # ìˆœì „íŒŒ ì‹œ xë¥¼ ê³±í–ˆìœ¼ë¯€ë¡œ
        return dx, dy
```

**ë¯¸ë¶„ ê·œì¹™**:
- $z = xy$ì¼ ë•Œ
- $\frac{\partial z}{\partial x} = y$
- $\frac{\partial z}{\partial y} = x$

#### 2. ë§ì…ˆ ê³„ì¸µ (Add Layer)
```python
class AddLayer:
    def forward(self, x, y):
        return x + y
    
    def backward(self, dout):
        dx = dout * 1  # ë§ì…ˆì˜ ë¯¸ë¶„ì€ 1
        dy = dout * 1
        return dx, dy
```

#### 3. ReLU ê³„ì¸µ
```python
class Relu:
    def __init__(self):
        self.mask = None  # True/False ë°°ì—´
    
    def forward(self, x):
        self.mask = (x <= 0)  # 0 ì´í•˜ì¸ ì›ì†Œ ìœ„ì¹˜ ì €ì¥
        out = x.copy()
        out[self.mask] = 0
        return out
    
    def backward(self, dout):
        dout[self.mask] = 0  # 0 ì´í•˜ì˜€ë˜ ê³³ì€ ê¸°ìš¸ê¸° 0
        return dout
```

> [!warning] ReLU vs Sigmoid ê¸°ìš¸ê¸°
> - **Sigmoid**: ì—­ì „íŒŒ ì‹œ $\sigma'(x)=\sigma(x)(1-\sigma(x))$ê°€ 0ì— ê°€ê¹Œì›Œì§€ë¯€ë¡œ ê¹Šì€ ë„¤íŠ¸ì›Œí¬ì—ì„œ *ê¸°ìš¸ê¸° ì†Œì‹¤* ë°œìƒ
> - **ReLU**: ì–‘ìˆ˜ ì˜ì—­ì—ì„œ ê¸°ìš¸ê¸° 1, ìŒìˆ˜ ì˜ì—­ì—ì„œ 0 â†’ ì†Œì‹¤ ì™„í™”í•˜ì§€ë§Œ `Dead ReLU`ì— ì£¼ì˜
> - ìµœì‹  ì‹œí—˜ì—ì„œëŠ” ReLU ê³„ì—´(Leaky, ELU)ë¡œ ì†Œì‹¤ ë¬¸ì œë¥¼ ì¤„ì´ëŠ” ì „ëµì„ ë¬»ëŠ” ê²½í–¥

#### 4. Sigmoid ê³„ì¸µ
```python
class Sigmoid:
    def __init__(self):
        self.out = None
    
    def forward(self, x):
        out = 1 / (1 + np.exp(-x))
        self.out = out
        return out
    
    def backward(self, dout):
        dx = dout * self.out * (1.0 - self.out)
        return dx
```

**ì‹œê·¸ëª¨ì´ë“œ ë¯¸ë¶„ ê³µì‹**:
$$\sigma'(x) = \sigma(x)(1 - \sigma(x))$$

#### 5. Affine ê³„ì¸µ (ì™„ì „ì—°ê²°ì¸µ)
```python
class Affine:
    def __init__(self, W, b):
        self.W = W
        self.b = b
        self.x = None
        self.dW = None
        self.db = None
    
    def forward(self, x):
        self.x = x
        out = np.dot(x, self.W) + self.b
        return out
    
    def backward(self, dout):
        dx = np.dot(dout, self.W.T)
        self.dW = np.dot(self.x.T, dout)
        self.db = np.sum(dout, axis=0)
        return dx
```

**í–‰ë ¬ ë¯¸ë¶„ ê·œì¹™**:
- $Y = XW + B$ì¼ ë•Œ
- $\frac{\partial L}{\partial X} = \frac{\partial L}{\partial Y} \cdot W^T$
- $\frac{\partial L}{\partial W} = X^T \cdot \frac{\partial L}{\partial Y}$
- $B$ëŠ” ê° ë°ì´í„°ì— **ë°˜ë³µ(add)ë˜ì—ˆìœ¼ë¯€ë¡œ** ì—­ì „íŒŒ ì‹œ ì¶• ë³„ í•©(sum)ìœ¼ë¡œ ê¸°ìš¸ê¸° ì§‘ê³„

> [!note] Batch ì…ë ¥ ì‹œ ì£¼ì˜
> - ì…ë ¥ $X$: `(ë°°ì¹˜í¬ê¸°, ì…ë ¥ì°¨ì›)`
> - ê°€ì¤‘ì¹˜ $W$: `(ì…ë ¥ì°¨ì›, ì¶œë ¥ì°¨ì›)`
> - í¸í–¥ $b$: `(ì¶œë ¥ì°¨ì›,)`
> - ì—­ì „íŒŒ ê²°ê³¼ `dx`: `(ë°°ì¹˜í¬ê¸°, ì…ë ¥ì°¨ì›)` í˜•íƒœ ìœ ì§€í•´ì•¼ í•¨

#### 6. Softmax-with-Loss ê³„ì¸µ
```python
class SoftmaxWithLoss:
    def __init__(self):
        self.loss = None
        self.y = None  # softmax ì¶œë ¥
        self.t = None  # ì •ë‹µ ë ˆì´ë¸”
    
    def forward(self, x, t):
        self.t = t
        self.y = softmax(x)
        self.loss = cross_entropy_error(self.y, self.t)
        return self.loss
    
    def backward(self, dout=1):
        batch_size = self.t.shape[0]
        dx = (self.y - self.t) / batch_size
        return dx
```

> [!tip] ì¤‘ìš”í•œ ì„±ì§ˆ
> Softmax + Cross Entropyì˜ ì—­ì „íŒŒëŠ” ë§¤ìš° ê°„ë‹¨!
> $$\frac{\partial L}{\partial a} = y - t$$
> - $y$: softmax ì¶œë ¥
> - $t$: ì •ë‹µ ë ˆì´ë¸” (ì›-í•«)

### ì „ì²´ ë„¤íŠ¸ì›Œí¬ ì—­ì „íŒŒ íë¦„ (2ì¸µ ì˜ˆì‹œ)

1. **Softmax-with-Loss**: $d\mathbf{a}^{(3)} = \mathbf{y} - \mathbf{t}$
2. **Affine(ì¶œë ¥ì¸µ)**: $dW^{(3)} = (\mathbf{a}^{(2)})^\top d\mathbf{a}^{(3)}$, $d\mathbf{a}^{(2)} = d\mathbf{a}^{(3)} (W^{(3)})^\top$
3. **í™œì„±í™” í•¨ìˆ˜ (ReLU/Sigmoid)**: ì›ì†Œë³„ë¡œ ê³± ë˜ëŠ” ë§ˆìŠ¤í‚¹
4. **Affine(ì€ë‹‰ì¸µ)** ë°˜ë³µ
5. **ì…ë ¥ì¸µ**ê¹Œì§€ ê¸°ìš¸ê¸° ì „ë‹¬

- ìˆœì„œ: `Affine â†’ ReLU â†’ Affine â†’ Softmax â†’ Cross Entropy` (ì•ì—ì„œ ë’¤ë¡œ)  
  ì—­ì „íŒŒ ì‹œì—ëŠ” **ë°˜ëŒ€ ìˆœì„œ**ë¡œ ê° ê³„ì¸µì˜ ì§€ì—­ ë¯¸ë¶„ì„ ì ìš©

### ê¸°ìš¸ê¸° ì†Œì‹¤ (Vanishing Gradient)

- ì—°ì†ëœ Sigmoid ê³„ì¸µì—ì„œëŠ” $\sigma'(x)\le 0.25$ â†’ ë‹¤ì¸µ ê³±ì…ˆ ì‹œ ê¸°ìš¸ê¸°ê°€ **ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ì‘ì•„ì§**
- Weight ì´ˆê¸°ê°’ì´ ì‘ê±°ë‚˜, ì¸µì´ ê¹Šì„ìˆ˜ë¡ í•™ìŠµì´ ì •ì²´ë˜ëŠ” ì›ì¸
- í•´ê²° ì „ëµ:
  - ReLU/Leaky ReLU/ELU ì‚¬ìš©
  - He/Xavier ì´ˆê¸°í™”
  - Batch Normalization
- > [!danger] ì‹œí—˜ì—ì„œ ***â€œSigmoidë§Œ ì‚¬ìš©í•œ ê¹Šì€ ë„¤íŠ¸ì›Œí¬ê°€ í•™ìŠµë˜ì§€ ì•ŠëŠ” ì´ìœ â€***ë¡œ ì§ì ‘ ì¶œì œ

---

## í•µì‹¬ ìˆ˜ì‹ ëª¨ìŒ

### í™œì„±í™” í•¨ìˆ˜

| í•¨ìˆ˜ | ìˆ˜ì‹ | ë¯¸ë¶„ |
|------|------|------|
| Sigmoid | $\sigma(x) = \frac{1}{1+e^{-x}}$ | $\sigma'(x) = \sigma(x)(1-\sigma(x))$ |
| ReLU | $\max(0, x)$ | $\begin{cases} 1 & (x>0) \\ 0 & (x\leq0) \end{cases}$ |
| Softmax | $y_k = \frac{e^{a_k}}{\sum_i e^{a_i}}$ | - |

### ì†ì‹¤ í•¨ìˆ˜

| í•¨ìˆ˜ | ìˆ˜ì‹ | ìš©ë„ |
|------|------|------|
| MSE | $E = \frac{1}{2}\sum(y-t)^2$ | íšŒê·€ |
| CEE | $E = -\sum t \log y$ | ë¶„ë¥˜ |

### ê²½ì‚¬ í•˜ê°•ë²•

$$W \leftarrow W - \eta \frac{\partial L}{\partial W}$$

### ì˜¤ì°¨ì—­ì „íŒŒ í•µì‹¬

$$\frac{\partial L}{\partial x} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial x}$$

---

## ì‹œí—˜ ì¶œì œ ì˜ˆìƒ ë¬¸ì œ

### 1. ê°œë… ë¬¸ì œ

> [!question] Q1. í¼ì…‰íŠ¸ë¡ ê³¼ ì‹ ê²½ë§ì˜ ì°¨ì´ì ì€?
> **ë‹µ**: 
> - í¼ì…‰íŠ¸ë¡ : ë‹¨ì¸µ êµ¬ì¡°, ì„ í˜• ë¶„ë¦¬ ê°€ëŠ¥í•œ ë¬¸ì œë§Œ í•´ê²°
> - ì‹ ê²½ë§: ë‹¤ì¸µ êµ¬ì¡°, ë¹„ì„ í˜• í™œì„±í™” í•¨ìˆ˜ ì‚¬ìš©, ë³µì¡í•œ ë¬¸ì œ í•´ê²° ê°€ëŠ¥

> [!question] Q2. ì™œ í™œì„±í™” í•¨ìˆ˜ëŠ” ë¹„ì„ í˜• í•¨ìˆ˜ì—¬ì•¼ í•˜ëŠ”ê°€?
> **ë‹µ**: ì„ í˜• í•¨ìˆ˜ë§Œ ì‚¬ìš©í•˜ë©´ ì¸µì„ ì•„ë¬´ë¦¬ ê¹Šê²Œ í•´ë„ í•˜ë‚˜ì˜ ì„ í˜• í•¨ìˆ˜ë¡œ í‘œí˜„ ê°€ëŠ¥. ë¹„ì„ í˜• í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ì•¼ ì¸µì„ ìŒ“ëŠ” ì˜ë¯¸ê°€ ìˆìŒ.

> [!question] Q3. í•™ìŠµ ì‹œ ì •í™•ë„ê°€ ì•„ë‹Œ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ëŠ” ì´ìœ ëŠ”?
> **ë‹µ**: ì •í™•ë„ëŠ” ëŒ€ë¶€ë¶„ì˜ ì¥ì†Œì—ì„œ ë¯¸ë¶„ê°’ì´ 0ì´ ë˜ì–´ ë§¤ê°œë³€ìˆ˜ë¥¼ ê°±ì‹ í•  ìˆ˜ ì—†ìŒ. ì†ì‹¤ í•¨ìˆ˜ëŠ” ì—°ì†ì ì´ê³  ë¯¸ë¶„ ê°€ëŠ¥í•˜ì—¬ ê²½ì‚¬í•˜ê°•ë²• ì ìš© ê°€ëŠ¥.

### 2. ê³„ì‚° ë¬¸ì œ

> [!question] Q4. AND ê²Œì´íŠ¸ì˜ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì„ ì„¤ì •í•˜ì‹œì˜¤.
> **ë‹µ**: 
> ```python
> w1 = 0.5, w2 = 0.5, b = -0.7
> # ë˜ëŠ” ë‹¤ë¥¸ ê°’ë„ ê°€ëŠ¥ (ì¡°ê±´: w1*1 + w2*1 + b > 0)
> ```

> [!question] Q5. ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ Ïƒ(x) = 1/(1+e^(-x))ì˜ ë¯¸ë¶„ì„ êµ¬í•˜ì‹œì˜¤.
> **ë‹µ**: Ïƒ'(x) = Ïƒ(x)(1 - Ïƒ(x))

### 3. êµ¬í˜„ ë¬¸ì œ

> [!question] Q6. ReLU í•¨ìˆ˜ë¥¼ NumPyë¡œ êµ¬í˜„í•˜ì‹œì˜¤.
> **ë‹µ**:
> ```python
> def relu(x):
>     return np.maximum(0, x)
> ```

> [!question] Q7. êµì°¨ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨ í•¨ìˆ˜ë¥¼ êµ¬í˜„í•˜ì‹œì˜¤.
> **ë‹µ**:
> ```python
> def cross_entropy_error(y, t):
>     delta = 1e-7
>     return -np.sum(t * np.log(y + delta))
> ```

### 4. ì˜¤ì°¨ì—­ì „íŒŒ ë¬¸ì œ

> [!question] Q8. ê³±ì…ˆ ê³„ì¸µì˜ ì—­ì „íŒŒë¥¼ ì„¤ëª…í•˜ì‹œì˜¤.
> **ë‹µ**:
> - ìˆœì „íŒŒ: z = x * y
> - ì—­ì „íŒŒ: 
>   - âˆ‚z/âˆ‚x = y â†’ dx = dout * y
>   - âˆ‚z/âˆ‚y = x â†’ dy = dout * x

> [!question] Q9. Affine ê³„ì¸µì—ì„œ ê°€ì¤‘ì¹˜ Wì˜ ê¸°ìš¸ê¸°ë¥¼ êµ¬í•˜ëŠ” ì‹ì€?
> **ë‹µ**: dW = X^T Â· dout

---

## í•µì‹¬ ì•”ê¸° ì‚¬í•­

> [!important] ë°˜ë“œì‹œ ì•”ê¸°!
> 
> ### í™œì„±í™” í•¨ìˆ˜ ì„ íƒ
> - **ì€ë‹‰ì¸µ**: ReLU (í˜„ì¬ í‘œì¤€)
> - **ì¶œë ¥ì¸µ (íšŒê·€)**: í•­ë“± í•¨ìˆ˜
> - **ì¶œë ¥ì¸µ (ì´ì§„ ë¶„ë¥˜)**: Sigmoid
> - **ì¶œë ¥ì¸µ (ë‹¤ì¤‘ ë¶„ë¥˜)**: Softmax
> 
> ### ì†ì‹¤ í•¨ìˆ˜ ì„ íƒ
> - **íšŒê·€ ë¬¸ì œ**: MSE (í‰ê·  ì œê³± ì˜¤ì°¨)
> - **ë¶„ë¥˜ ë¬¸ì œ**: CEE (êµì°¨ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨)
> 
> ### í•™ìŠµ ê³¼ì •
> 1. ë¯¸ë‹ˆë°°ì¹˜: í›ˆë ¨ ë°ì´í„° ì¤‘ ì¼ë¶€ë¥¼ ë¬´ì‘ìœ„ë¡œ ì„ íƒ
> 2. ê¸°ìš¸ê¸° ê³„ì‚°: ì†ì‹¤ í•¨ìˆ˜ì˜ ë¯¸ë¶„ ê³„ì‚°
> 3. ë§¤ê°œë³€ìˆ˜ ê°±ì‹ : W â† W - Î·Â·âˆ‡W
> 4. ë°˜ë³µ
> 
> ### ì˜¤ì°¨ì—­ì „íŒŒë²• ì¥ì 
> - ìˆ˜ì¹˜ ë¯¸ë¶„ë³´ë‹¤ í›¨ì”¬ ë¹ ë¦„
> - ë§¤ê°œë³€ìˆ˜ê°€ ë§ì•„ë„ íš¨ìœ¨ì ìœ¼ë¡œ ê³„ì‚° ê°€ëŠ¥

---

## ì¶”ê°€ í•™ìŠµ ìë£Œ

- [[Ch2. í¼ì…‰íŠ¸ë¡  ìƒì„¸ ì •ë¦¬]]
- [[Ch4. ê²½ì‚¬ í•˜ê°•ë²• ìµœì í™”]]
- [[Ch5. ì˜¤ì°¨ì—­ì „íŒŒë²• ìˆ˜í•™ì  ì¦ëª…]]

---

> [!success] ì‹œí—˜ ê³µë¶€ ì²´í¬ë¦¬ìŠ¤íŠ¸
> - [ ] ê° í™œì„±í™” í•¨ìˆ˜ì˜ íŠ¹ì§•ê³¼ ë¯¸ë¶„ ê³µì‹ ì•”ê¸°
> - [ ] ì†ì‹¤ í•¨ìˆ˜ ê³µì‹ê³¼ ìš©ë„ ì•”ê¸°
> - [ ] í¼ì…‰íŠ¸ë¡ ìœ¼ë¡œ XORë¥¼ êµ¬í˜„í•  ìˆ˜ ì—†ëŠ” ì´ìœ  ì´í•´
> - [ ] ê²½ì‚¬ í•˜ê°•ë²• ìˆ˜ì‹ ì•”ê¸°
> - [ ] ì£¼ìš” ê³„ì¸µì˜ ìˆœì „íŒŒ/ì—­ì „íŒŒ êµ¬í˜„ ì´í•´
> - [ ] Softmax-with-Lossì˜ ì—­ì „íŒŒê°€ y-tì¸ ì´ìœ  ì´í•´
