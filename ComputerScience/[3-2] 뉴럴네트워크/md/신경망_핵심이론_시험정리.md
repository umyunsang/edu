---
tags: [neural-network, deep-learning, exam, summary]
aliases: [신경망 시험 정리, NN Exam Notes]
created: 2025-10-18
---

# 신경망 핵심 이론 정리 (시험 대비)

> [!info] 문서 정보
> - 과목: 신경망 (Neural Network)
> - 목적: 시험 대비 핵심 개념 정리
> - 구성: 파이썬 기초 → 퍼셉트론 → 신경망 → 학습 알고리즘 → 오차역전파법

---

## 📚 목차

1. [[#Ch1. 파이썬 기초]]
2. [[#Ch2. 퍼셉트론 (Perceptron)]]
3. [[#Ch3. 신경망 (Neural Network)]]
4. [[#Ch4. 학습 알고리즘]]
5. [[#Ch5. 오차역전파법 (Backpropagation)]]
6. [[#핵심 수식 모음]]
7. [[#시험 출제 예상 문제]]

---

## Ch1. 파이썬 기초

### NumPy 핵심 개념

#### 브로드캐스팅 (Broadcasting)
```python
# 스칼라와 배열 연산
x = np.array([1, 2, 3])
x * 2  # [2, 4, 6] - 자동으로 모든 원소에 적용

# 다차원 배열 브로드캐스팅
A = np.array([[1, 2], [3, 4]])
B = np.array([10, 20])
A * B  # [[10, 40], [30, 80]] - B가 각 행에 적용됨
```

> [!tip] 암기 포인트
> - **브로드캐스팅**: 형상이 다른 배열 간 연산 시 자동으로 형상을 맞춰줌
> - **원소별 연산 (element-wise)**: `*`, `+`, `-`, `/` 등은 원소별로 계산
> - **행렬 곱**: `np.dot(A, B)` 사용 (내적)

#### NumPy 배열 연산 정리

| 연산 | 코드 | 설명 |
|------|------|------|
| 원소별 곱 | `A * B` | element-wise product |
| 행렬 곱 | `np.dot(A, B)` | matrix multiplication |
| 전치 행렬 | `A.T` 또는 `np.transpose(A)` | transpose |
| 평탄화 | `A.flatten()` | 1차원 배열로 변환 |

---

## Ch2. 퍼셉트론 (Perceptron)

### 퍼셉트론의 정의

> [!important] 핵심 개념
> **퍼셉트론**: 다수의 신호를 입력받아 하나의 신호를 출력하는 알고리즘
> - 입력: $x_1, x_2, ..., x_n$
> - 가중치: $w_1, w_2, ..., w_n$
> - 편향: $b$
> - 출력: $y = \begin{cases} 0 & (w_1x_1 + w_2x_2 + b \leq 0) \\ 1 & (w_1x_1 + w_2x_2 + b > 0) \end{cases}$

### 가중치와 편향

- **가중치 (weight, w)**: 각 입력 신호의 중요도를 나타냄
  - 가중치가 클수록 해당 입력이 중요함
  
- **편향 (bias, b)**: 뉴런이 얼마나 쉽게 활성화되는지 조정
  - 편향이 클수록 쉽게 활성화됨
  - 편향이 작을수록 활성화 어려움

### 논리 게이트 구현

#### AND 게이트
```python
def AND(x1, x2):
    w1, w2, b = 0.5, 0.5, -0.7
    tmp = w1*x1 + w2*x2 + b
    return 1 if tmp > 0 else 0
```

| x1 | x2 | y |
|----|----|---|
| 0  | 0  | 0 |
| 0  | 1  | 0 |
| 1  | 0  | 0 |
| 1  | 1  | 1 |

#### OR 게이트
```python
def OR(x1, x2):
    w1, w2, b = 0.5, 0.5, -0.2  # 편향이 AND보다 큼
    tmp = w1*x1 + w2*x2 + b
    return 1 if tmp > 0 else 0
```

#### NAND 게이트 (NOT AND)
```python
def NAND(x1, x2):
    w1, w2, b = -0.5, -0.5, 0.7  # 가중치가 음수
    tmp = w1*x1 + w2*x2 + b
    return 1 if tmp > 0 else 0
```

### 다층 퍼셉트론 (Multi-Layer Perceptron)

> [!warning] 중요 개념
> **XOR 게이트는 단층 퍼셉트론으로 구현 불가능**
> - 이유: 선형 분리 불가능 (비선형 문제)
> - 해결: 다층 퍼셉트론 필요

#### XOR 게이트 (2층 구조)
```python
def XOR(x1, x2):
    s1 = NAND(x1, x2)  # 은닉층 1
    s2 = OR(x1, x2)    # 은닉층 2
    y = AND(s1, s2)    # 출력층
    return y
```

**XOR 진리표**:

| x1 | x2 | NAND | OR | AND (출력) |
|----|----|------|----|----|
| 0  | 0  | 1    | 0  | 0  |
| 0  | 1  | 1    | 1  | 1  |
| 1  | 0  | 1    | 1  | 1  |
| 1  | 1  | 0    | 1  | 0  |

---

## Ch3. 신경망 (Neural Network)

### 활성화 함수 (Activation Function)

> [!important] 왜 활성화 함수가 필요한가?
> - 선형 함수만 사용하면 층을 아무리 깊게 해도 단층과 동일
> - 비선형 함수를 사용해야 층을 쌓는 의미가 있음

#### 1. 계단 함수 (Step Function)
```python
def step_function(x):
    return np.array(x > 0, dtype=int)
```
- 임계값을 기준으로 0 또는 1 출력
- **단점**: 미분 불가능 → 학습 불가

#### 2. 시그모이드 함수 (Sigmoid Function)
$$\sigma(x) = \frac{1}{1 + e^{-x}}$$

```python
def sigmoid(x):
    return 1 / (1 + np.exp(-x))
```

**특징**:
- 출력 범위: (0, 1)
- 매끄러운 곡선
- 미분 가능
- **단점**: 기울기 소실 문제 (gradient vanishing)

**미분**:
$$\sigma'(x) = \sigma(x)(1 - \sigma(x))$$

#### 3. ReLU 함수 (Rectified Linear Unit)
$$\text{ReLU}(x) = \max(0, x)$$

```python
def relu(x):
    return np.maximum(0, x)
```

**특징**:
- $x > 0$: 출력 = x (기울기 1)
- $x \leq 0$: 출력 = 0 (기울기 0)
- **장점**: 
  - 기울기 소실 문제 해결
  - 계산 효율적
  - 현재 가장 많이 사용됨

**미분**:
$$\text{ReLU}'(x) = \begin{cases} 1 & (x > 0) \\ 0 & (x \leq 0) \end{cases}$$

#### 4. 하이퍼볼릭 탄젠트 (Tanh)
$$\tanh(x)=\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}$$

**특징**:
- 출력 범위가 $(-1, 1)$로 시그모이드보다 중심이 0에 가까워 수렴이 빠른 편
- $\tanh(x)=2\cdot\text{sigmoid}(2x)-1$ 관계 기억하기

#### 5. Leaky ReLU
$$f(x)=\begin{cases}x & x\ge0 \\ \alpha x & x<0\end{cases}$$

**포인트**:
- ReLU의 $0$ 이하 구간 기울기 0 문제를 $\alpha(0<\alpha\ll1)$로 완화
- 시험에서는 $\alpha$가 작은 상수임을 확인시키는 문제가 자주 나옴

#### 6. ELU (Exponential Linear Unit)
$$f(x)=\begin{cases}x & x\ge0 \\ \alpha\left(e^{x}-1\right) & x<0\end{cases}$$

**특징**:
- $x<0$ 구간도 부드럽고 미분 가능
- 평균 출력이 0에 가까워 학습 안정화에 도움

#### 7. Swish
$$\text{swish}(x) = x\cdot \text{sigmoid}(x)$$

**특징**:
- $x<0$ 영역에서도 완만하게 감소 → 깊은 네트워크에서 성능 우수
- 구글에서 제안, 시험에서는 시그모이드와의 곱 구조를 묻는 단답형 가능

#### 8. GELU (Gaussian Error Linear Unit)
$$\text{GELU}(x)=x\cdot \Phi(x)$$

**포인트**:
- $\Phi(x)$: 표준 정규분포의 누적분포함수(CDF)
- 최근 Transformer 계열에서 기본 활성화 함수 → 현대 신경망 트렌드 대비 문제

### 출력층 활성화 함수

#### 소프트맥스 함수 (Softmax)
$$y_k = \frac{\exp(a_k)}{\sum_{i=1}^{n} \exp(a_i)}$$

```python
def softmax(x):
    c = np.max(x)  # 오버플로 방지
    exp_x = np.exp(x - c)
    return exp_x / np.sum(exp_x)
```

**특징**:
- 다중 클래스 분류에 사용
- 출력의 합이 1 (확률 분포)
- 오버플로 방지를 위해 최댓값을 빼줌

> [!tip] 출력층 활성화 함수 선택
> - **회귀 문제**: 항등 함수 (identity function)
> - **이진 분류**: 시그모이드
> - **다중 분류**: 소프트맥스

> [!note] Softmax 핵심 성질
> - 모든 출력이 0 이상이며 합이 1 → 확률 벡터 형성
> - $ \text{softmax}(\mathbf{a}+c)=\text{softmax}(\mathbf{a}) $ (모든 성분에 동일 상수 $c$ 추가 시 결과 동일)
> - 실전 계산 시 $\max(\mathbf{a})$를 빼서 수치적 안정성 확보

### 3층 신경망 구조 & 행렬 표현

- 입력 $\mathbf{x}$ → 은닉층 1 → 은닉층 2 → 출력층으로 **Affine → 활성화** 조합 반복
- 각 층 순전파:
  - 1층: $\mathbf{z}^{(1)} = W^{(1)}\mathbf{x} + \mathbf{b}^{(1)}$, $\mathbf{a}^{(1)} = h(\mathbf{z}^{(1)})$
  - 2층: $\mathbf{z}^{(2)} = W^{(2)}\mathbf{a}^{(1)} + \mathbf{b}^{(2)}$, $\mathbf{a}^{(2)} = h(\mathbf{z}^{(2)})$
  - 출력층: $\mathbf{l} = W^{(3)}\mathbf{a}^{(2)} + \mathbf{b}^{(3)}$, $\mathbf{y} = \text{softmax}(\mathbf{l})$
- **행렬 차원 시험 포인트**:
  - $W^{(l)}$: (해당 층 뉴런 수 × 이전 층 뉴런 수)
  - $\mathbf{b}^{(l)}$: (해당 층 뉴런 수 × 1)
  - 행렬 곱이 가능하려면 열·행 크기 일치 확인 필수
- 흔히 사용하는 구성: `Affine → Sigmoid → Affine → Sigmoid → Affine → Softmax`

### MNIST 데이터셋 핵심 스펙

- 숫자 0~9 손글씨 이미지, **훈련 60,000장 / 테스트 10,000장**
- 각 이미지는 $28 \times 28$ 픽셀, 한 픽셀 밝기는 0~255
- 입력 벡터는 784차원(=28×28)으로 평탄화하여 사용
- 라벨은 10차원 원-핫 인코딩으로 표현

> [!important] 시험 대비: 데이터 크기·분할·해상도는 빈출 암기 요소

---

## Ch4. 학습 알고리즘

### 라벨 전처리 (One-hot Encoding)

- 분류 문제에서는 정답 레이블을 원-핫 벡터로 변환
  - 예: 숫자 3 → `[0,0,0,1,0,0,0,0,0,0]`
  - 예: 숫자 7 → `[0,0,0,0,0,0,0,1,0,0]`
- 소프트맥스 출력과 동일 길이로 맞춰 손실 함수를 계산 가능하게 함

> [!important] 시험 체크
> - 각 자릿수에 대응하는 클래스 위치를 정확히 매핑해야 함
> - 라벨 벡터는 확률 분포가 아니지만, 정답 클래스에만 1을 두는 구조라는 점을 묻는 문제가 잦음

### 손실 함수 (Loss Function)

> [!important] 손실 함수의 역할
> - 신경망의 성능을 "나쁨의 정도"로 측정
> - 학습의 목표: 손실 함수를 최소화하는 가중치 찾기
> - 정확도가 아닌 손실 함수를 사용하는 이유: 미분 가능해야 함

#### 1. 평균 제곱 오차 (MSE, Mean Squared Error)
$$E = \frac{1}{2}\sum_{k}(y_k - t_k)^2$$

```python
def mean_squared_error(y, t):
    return 0.5 * np.sum((y - t)**2)
```

- 주로 **회귀 문제**에 사용
- $y$: 신경망 출력, $t$: 정답 레이블

#### 2. 교차 엔트로피 오차 (CEE, Cross Entropy Error)
$$E = -\sum_{k} t_k \log y_k$$

```python
def cross_entropy_error(y, t):
    delta = 1e-7  # log(0) 방지
    return -np.sum(t * np.log(y + delta))
```

- 주로 **분류 문제**에 사용
- $t$: 원-핫 인코딩된 정답 레이블
- $y$: 소프트맥스 출력 (확률)

> [!warning] 시험 출제 포인트
> - MSE는 회귀, CEE는 분류
> - CEE에서 `delta`를 더하는 이유: log(0) = -∞ 방지
> - 원-핫 인코딩: [0, 0, 1, 0, 0] 형태

> [!note] 데이터셋 손실 산출
> - 단일 샘플 손실을 구한 뒤 **전체 데이터 평균**으로 J(w) 계산
> - $E_{\text{dataset}} = \frac{1}{N}\sum_{n=1}^{N}E^{(n)}$ 형태라는 점을 기억

### 미니배치 학습

```python
train_size = x_train.shape[0]
batch_size = 100
batch_mask = np.random.choice(train_size, batch_size)
x_batch = x_train[batch_mask]
t_batch = t_train[batch_mask]
```

- 전체 데이터가 아닌 일부(미니배치)만 사용
- **장점**: 메모리 효율적, 학습 속도 향상

### 경사 하강법 (Gradient Descent)

$$W \leftarrow W - \eta \frac{\partial L}{\partial W}$$

- $W$: 가중치
- $\eta$: 학습률 (learning rate)
- $\frac{\partial L}{\partial W}$: 손실 함수의 기울기

```python
def gradient_descent(f, init_x, lr=0.01, step_num=100):
    x = init_x
    for i in range(step_num):
        grad = numerical_gradient(f, x)
        x -= lr * grad  # 가중치 갱신
    return x
```

- 일반식: $\mathbf{w}_{k+1} = \mathbf{w}_{k} - \eta \nabla J(\mathbf{w}_{k})$
- 기울기는 손실이 가장 크게 증가하는 방향이므로 **반대 방향으로 이동**해야 최소값에 접근

> [!important] 학습률 설정
> - **너무 크면**: 발산 (학습 불안정)
> - **너무 작으면**: 학습 매우 느림
> - **적절한 값**: 0.01 ~ 0.001 정도

> [!warning] 초기값 & 최소점
> - 잘못된 초기 가중치는 **local minima** 또는 **saddle point**에 빠질 수 있음
> - 실전에서는 가중치 초기화 전략(He, Xavier 등)과 학습률 스케줄을 함께 설계

### 확률적 경사 하강법 (SGD)

```python
for epoch in range(num_epochs):
    np.random.shuffle(mini_batches)
    for x_batch, t_batch in mini_batches:
        grad = gradient(x_batch, t_batch)
        W -= lr * grad
```

- 전체 데이터 대신 **무작위 미니배치**를 사용해 매 업데이트 수행
- 장점: 계산량 감소, 지역 최소점 탈출 가능성 증가, 온라인 학습 적합
- 용어 매칭: 확률적 경사 하강법 = **Stochastic Gradient Descent**

### 변수 개수 계산 예시

- 2층 신경망 (784 → 50 → 10) 매개변수 수:
  - 가중치: $784 \times 50 + 50 \times 10$
  - 편향: $50 + 10$
  - **총 39,760개**
- > [!tip] 시험에서 자주 요구되는 빠른 계산 예시 → 층별 뉴런 수 곱에 편향을 더하면 됨

### 수치 미분 vs 해석적 미분

#### 수치 미분 (Numerical Differentiation)
$$\frac{df(x)}{dx} \approx \frac{f(x+h) - f(x-h)}{2h}$$

```python
def numerical_gradient(f, x):
    h = 1e-4  # 0.0001
    grad = np.zeros_like(x)
    
    for idx in range(x.size):
        tmp_val = x[idx]
        # f(x+h)
        x[idx] = tmp_val + h
        fxh1 = f(x)
        # f(x-h)
        x[idx] = tmp_val - h
        fxh2 = f(x)
        # 중심 차분법
        grad[idx] = (fxh1 - fxh2) / (2*h)
        x[idx] = tmp_val
    
    return grad
```

- **장점**: 구현 간단, 정확
- **단점**: 계산 느림
- **용도**: 오차역전파법 검증 (gradient check)

> [!danger] 계산 비용 폭발
> - 매개변수 39,760개 × 10,000번 반복 학습이라면 **수치 미분만**으로는 계산 불가 수준
> - 합성 함수(affine, sigmoid, softmax, CEE)는 **chain rule 기반 해석적 미분(=오차역전파)**으로 처리해야 실시간 학습 가능

---

## Ch5. 오차역전파법 (Backpropagation)

### 계산 그래프

> [!important] 계산 그래프 개념
> - 계산 과정을 그래프로 표현
> - **순전파 (Forward)**: 입력 → 출력 계산
> - **역전파 (Backward)**: 미분(기울기) 계산

### 연쇄 법칙 (Chain Rule)

$$\frac{\partial z}{\partial x} = \frac{\partial z}{\partial y} \cdot \frac{\partial y}{\partial x}$$

- 합성 함수의 미분은 각 함수 미분의 곱

> [!important] Chain Rule 암기 포인트
> - **순전파**: 노드 출력을 차례차례 계산 (예: 가격 × 수량 → 세금 곱)
> - **역전파**: 출력 기울기를 입력 기울기로 되돌려보내며 *지역 미분*을 곱함
> - 하나의 노드에 여러 경로가 모이면 기울기를 **합산**하고, 나뉘면 각 경로로 **복사**됨

### 기본 노드 역전파 규칙 (빈출)

| 노드 | 순전파 | 역전파 |
|------|--------|--------|
| 덧셈 | $z = x + y$ | $\frac{\partial L}{\partial x} = \frac{\partial L}{\partial z}$<br>$\frac{\partial L}{\partial y} = \frac{\partial L}{\partial z}$ |
| 곱셈 | $z = xy$ | $\frac{\partial L}{\partial x} = y \cdot \frac{\partial L}{\partial z}$<br>$\frac{\partial L}{\partial y} = x \cdot \frac{\partial L}{\partial z}$ |
| 역수 | $z = 1/x$ | $\frac{\partial L}{\partial x} = -x^{-2} \cdot \frac{\partial L}{\partial z}$ |
| 거듭제곱 | $z = x^2$ | $\frac{\partial L}{\partial x} = 2x \cdot \frac{\partial L}{\partial z}$ |
| exp | $z = e^{x}$ | $\frac{\partial L}{\partial x} = e^{x} \cdot \frac{\partial L}{\partial z}$ |
| log | $z = \log x$ | $\frac{\partial L}{\partial x} = \frac{1}{x} \cdot \frac{\partial L}{\partial z}$ |

> [!tip] 시험 팁  
> - **곱셈 노드**: 반대편 값을 곱한다  
> - **덧셈 노드**: 그대로 전달한다  
> - 역수/로그/지수 노드는 미분 공식 그대로 외워두기

### 계산 그래프 예시 (사과·귤 장바구니)

- 순전파: `(사과가격 × 사과수 + 귤가격 × 귤수) × (1 + 소비세)` 구조
- 역전파 흐름:
  1. 최종 금액에 대한 기울기 1에서 시작
  2. 곱셈 노드 → 세금 항 $1+\text{세율}$을 곱해 각 항목으로 전달
  3. 덧셈 노드 → 사과 비용과 귤 비용 방향으로 동일 기울기 복사
  4. 각 곱셈 노드 → 수량/가격 값을 곱해 상대 노드에 전달
- > [!important] 계산 그래프 사용 목적: 복잡한 합성 함수를 **노드 단위로 쪼개** 역전파를 체계적으로 수행

### 주요 계층 (Layer) 구현

#### 1. 곱셈 계층 (Mul Layer)
```python
class MulLayer:
    def __init__(self):
        self.x = None
        self.y = None
    
    def forward(self, x, y):
        self.x = x
        self.y = y
        return x * y
    
    def backward(self, dout):
        dx = dout * self.y  # 순전파 시 y를 곱했으므로
        dy = dout * self.x  # 순전파 시 x를 곱했으므로
        return dx, dy
```

**미분 규칙**:
- $z = xy$일 때
- $\frac{\partial z}{\partial x} = y$
- $\frac{\partial z}{\partial y} = x$

#### 2. 덧셈 계층 (Add Layer)
```python
class AddLayer:
    def forward(self, x, y):
        return x + y
    
    def backward(self, dout):
        dx = dout * 1  # 덧셈의 미분은 1
        dy = dout * 1
        return dx, dy
```

#### 3. ReLU 계층
```python
class Relu:
    def __init__(self):
        self.mask = None  # True/False 배열
    
    def forward(self, x):
        self.mask = (x <= 0)  # 0 이하인 원소 위치 저장
        out = x.copy()
        out[self.mask] = 0
        return out
    
    def backward(self, dout):
        dout[self.mask] = 0  # 0 이하였던 곳은 기울기 0
        return dout
```

> [!warning] ReLU vs Sigmoid 기울기
> - **Sigmoid**: 역전파 시 $\sigma'(x)=\sigma(x)(1-\sigma(x))$가 0에 가까워지므로 깊은 네트워크에서 *기울기 소실* 발생
> - **ReLU**: 양수 영역에서 기울기 1, 음수 영역에서 0 → 소실 완화하지만 `Dead ReLU`에 주의
> - 최신 시험에서는 ReLU 계열(Leaky, ELU)로 소실 문제를 줄이는 전략을 묻는 경향

#### 4. Sigmoid 계층
```python
class Sigmoid:
    def __init__(self):
        self.out = None
    
    def forward(self, x):
        out = 1 / (1 + np.exp(-x))
        self.out = out
        return out
    
    def backward(self, dout):
        dx = dout * self.out * (1.0 - self.out)
        return dx
```

**시그모이드 미분 공식**:
$$\sigma'(x) = \sigma(x)(1 - \sigma(x))$$

#### 5. Affine 계층 (완전연결층)
```python
class Affine:
    def __init__(self, W, b):
        self.W = W
        self.b = b
        self.x = None
        self.dW = None
        self.db = None
    
    def forward(self, x):
        self.x = x
        out = np.dot(x, self.W) + self.b
        return out
    
    def backward(self, dout):
        dx = np.dot(dout, self.W.T)
        self.dW = np.dot(self.x.T, dout)
        self.db = np.sum(dout, axis=0)
        return dx
```

**행렬 미분 규칙**:
- $Y = XW + B$일 때
- $\frac{\partial L}{\partial X} = \frac{\partial L}{\partial Y} \cdot W^T$
- $\frac{\partial L}{\partial W} = X^T \cdot \frac{\partial L}{\partial Y}$
- $B$는 각 데이터에 **반복(add)되었으므로** 역전파 시 축 별 합(sum)으로 기울기 집계

> [!note] Batch 입력 시 주의
> - 입력 $X$: `(배치크기, 입력차원)`
> - 가중치 $W$: `(입력차원, 출력차원)`
> - 편향 $b$: `(출력차원,)`
> - 역전파 결과 `dx`: `(배치크기, 입력차원)` 형태 유지해야 함

#### 6. Softmax-with-Loss 계층
```python
class SoftmaxWithLoss:
    def __init__(self):
        self.loss = None
        self.y = None  # softmax 출력
        self.t = None  # 정답 레이블
    
    def forward(self, x, t):
        self.t = t
        self.y = softmax(x)
        self.loss = cross_entropy_error(self.y, self.t)
        return self.loss
    
    def backward(self, dout=1):
        batch_size = self.t.shape[0]
        dx = (self.y - self.t) / batch_size
        return dx
```

> [!tip] 중요한 성질
> Softmax + Cross Entropy의 역전파는 매우 간단!
> $$\frac{\partial L}{\partial a} = y - t$$
> - $y$: softmax 출력
> - $t$: 정답 레이블 (원-핫)

### 전체 네트워크 역전파 흐름 (2층 예시)

1. **Softmax-with-Loss**: $d\mathbf{a}^{(3)} = \mathbf{y} - \mathbf{t}$
2. **Affine(출력층)**: $dW^{(3)} = (\mathbf{a}^{(2)})^\top d\mathbf{a}^{(3)}$, $d\mathbf{a}^{(2)} = d\mathbf{a}^{(3)} (W^{(3)})^\top$
3. **활성화 함수 (ReLU/Sigmoid)**: 원소별로 곱 또는 마스킹
4. **Affine(은닉층)** 반복
5. **입력층**까지 기울기 전달

- 순서: `Affine → ReLU → Affine → Softmax → Cross Entropy` (앞에서 뒤로)  
  역전파 시에는 **반대 순서**로 각 계층의 지역 미분을 적용

### 기울기 소실 (Vanishing Gradient)

- 연속된 Sigmoid 계층에서는 $\sigma'(x)\le 0.25$ → 다층 곱셈 시 기울기가 **기하급수적으로 작아짐**
- Weight 초기값이 작거나, 층이 깊을수록 학습이 정체되는 원인
- 해결 전략:
  - ReLU/Leaky ReLU/ELU 사용
  - He/Xavier 초기화
  - Batch Normalization
- > [!danger] 시험에서 ***“Sigmoid만 사용한 깊은 네트워크가 학습되지 않는 이유”***로 직접 출제

---

## 핵심 수식 모음

### 활성화 함수

| 함수 | 수식 | 미분 |
|------|------|------|
| Sigmoid | $\sigma(x) = \frac{1}{1+e^{-x}}$ | $\sigma'(x) = \sigma(x)(1-\sigma(x))$ |
| ReLU | $\max(0, x)$ | $\begin{cases} 1 & (x>0) \\ 0 & (x\leq0) \end{cases}$ |
| Softmax | $y_k = \frac{e^{a_k}}{\sum_i e^{a_i}}$ | - |

### 손실 함수

| 함수 | 수식 | 용도 |
|------|------|------|
| MSE | $E = \frac{1}{2}\sum(y-t)^2$ | 회귀 |
| CEE | $E = -\sum t \log y$ | 분류 |

### 경사 하강법

$$W \leftarrow W - \eta \frac{\partial L}{\partial W}$$

### 오차역전파 핵심

$$\frac{\partial L}{\partial x} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial x}$$

---

## 시험 출제 예상 문제

### 1. 개념 문제

> [!question] Q1. 퍼셉트론과 신경망의 차이점은?
> **답**: 
> - 퍼셉트론: 단층 구조, 선형 분리 가능한 문제만 해결
> - 신경망: 다층 구조, 비선형 활성화 함수 사용, 복잡한 문제 해결 가능

> [!question] Q2. 왜 활성화 함수는 비선형 함수여야 하는가?
> **답**: 선형 함수만 사용하면 층을 아무리 깊게 해도 하나의 선형 함수로 표현 가능. 비선형 함수를 사용해야 층을 쌓는 의미가 있음.

> [!question] Q3. 학습 시 정확도가 아닌 손실 함수를 사용하는 이유는?
> **답**: 정확도는 대부분의 장소에서 미분값이 0이 되어 매개변수를 갱신할 수 없음. 손실 함수는 연속적이고 미분 가능하여 경사하강법 적용 가능.

### 2. 계산 문제

> [!question] Q4. AND 게이트의 가중치와 편향을 설정하시오.
> **답**: 
> ```python
> w1 = 0.5, w2 = 0.5, b = -0.7
> # 또는 다른 값도 가능 (조건: w1*1 + w2*1 + b > 0)
> ```

> [!question] Q5. 시그모이드 함수 σ(x) = 1/(1+e^(-x))의 미분을 구하시오.
> **답**: σ'(x) = σ(x)(1 - σ(x))

### 3. 구현 문제

> [!question] Q6. ReLU 함수를 NumPy로 구현하시오.
> **답**:
> ```python
> def relu(x):
>     return np.maximum(0, x)
> ```

> [!question] Q7. 교차 엔트로피 오차 함수를 구현하시오.
> **답**:
> ```python
> def cross_entropy_error(y, t):
>     delta = 1e-7
>     return -np.sum(t * np.log(y + delta))
> ```

### 4. 오차역전파 문제

> [!question] Q8. 곱셈 계층의 역전파를 설명하시오.
> **답**:
> - 순전파: z = x * y
> - 역전파: 
>   - ∂z/∂x = y → dx = dout * y
>   - ∂z/∂y = x → dy = dout * x

> [!question] Q9. Affine 계층에서 가중치 W의 기울기를 구하는 식은?
> **답**: dW = X^T · dout

---

## 핵심 암기 사항

> [!important] 반드시 암기!
> 
> ### 활성화 함수 선택
> - **은닉층**: ReLU (현재 표준)
> - **출력층 (회귀)**: 항등 함수
> - **출력층 (이진 분류)**: Sigmoid
> - **출력층 (다중 분류)**: Softmax
> 
> ### 손실 함수 선택
> - **회귀 문제**: MSE (평균 제곱 오차)
> - **분류 문제**: CEE (교차 엔트로피 오차)
> 
> ### 학습 과정
> 1. 미니배치: 훈련 데이터 중 일부를 무작위로 선택
> 2. 기울기 계산: 손실 함수의 미분 계산
> 3. 매개변수 갱신: W ← W - η·∇W
> 4. 반복
> 
> ### 오차역전파법 장점
> - 수치 미분보다 훨씬 빠름
> - 매개변수가 많아도 효율적으로 계산 가능

---

## 추가 학습 자료

- [[Ch2. 퍼셉트론 상세 정리]]
- [[Ch4. 경사 하강법 최적화]]
- [[Ch5. 오차역전파법 수학적 증명]]

---

> [!success] 시험 공부 체크리스트
> - [ ] 각 활성화 함수의 특징과 미분 공식 암기
> - [ ] 손실 함수 공식과 용도 암기
> - [ ] 퍼셉트론으로 XOR를 구현할 수 없는 이유 이해
> - [ ] 경사 하강법 수식 암기
> - [ ] 주요 계층의 순전파/역전파 구현 이해
> - [ ] Softmax-with-Loss의 역전파가 y-t인 이유 이해
